{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5e32153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n在大改模型后训练模型的时段同步进行可视化部分的修改\\n主要目标是加入CRF\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "在大改模型后训练模型的时段同步进行可视化部分的修改\n",
    "主要目标是加入CRF\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db777f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录： C:\\Users\\HP\n",
      "修改后的工作目录： D:\\李娅宁\\肩台外侧点-0715\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "统一设置地址\n",
    "'''\n",
    "\n",
    "import os\n",
    "\n",
    "# 获取当前工作目录\n",
    "current_dir = os.getcwd()\n",
    "print(\"当前工作目录：\", current_dir)\n",
    "\n",
    "# 修改当前工作目录，以后输出文件只需要写文件名\n",
    "new_dir = \"D:/李娅宁/肩台外侧点-0715/\"\n",
    "os.chdir(new_dir)\n",
    "print(\"修改后的工作目录：\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ef41ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automat==20.2.0\n",
      "Babel==2.11.0\n",
      "Bottleneck==1.3.5\n",
      "ConfigArgParse==1.7\n",
      "Cython==3.0.10\n",
      "Flask==2.2.2\n",
      "HeapDict==1.0.1\n",
      "Jinja2==3.1.2\n",
      "Markdown==3.4.1\n",
      "MarkupSafe==2.1.1\n",
      "Pillow==10.0.1\n",
      "Protego==0.1.16\n",
      "PyDispatcher==2.0.5\n",
      "PyJWT==2.4.0\n",
      "PyNaCl==1.5.0\n",
      "PyQt5==5.15.7\n",
      "PyQt5-sip==12.11.0\n",
      "PyQtWebEngine==5.15.4\n",
      "PySocks==1.7.1\n",
      "PyWavelets==1.4.1\n",
      "PyYAML==6.0\n",
      "Pygments==2.15.1\n",
      "QDarkStyle==3.0.2\n",
      "QtAwesome==1.2.2\n",
      "QtPy==2.2.0\n",
      "Rtree==1.0.1\n",
      "SQLAlchemy==1.4.39\n",
      "Scrapy==2.8.0\n",
      "Send2Trash==1.8.0\n",
      "Sphinx==5.0.2\n",
      "TBB==0.2\n",
      "Twisted==22.10.0\n",
      "Unidecode==1.2.0\n",
      "Werkzeug==2.2.3\n",
      "absl-py==2.1.0\n",
      "aiobotocore==2.5.0\n",
      "aiofiles==22.1.0\n",
      "aiohttp==3.8.5\n",
      "aioitertools==0.7.1\n",
      "aiosignal==1.2.0\n",
      "aiosqlite==0.18.0\n",
      "alabaster==0.7.12\n",
      "anaconda-anon-usage==0.4.2\n",
      "anaconda-catalogs==0.2.0\n",
      "anaconda-client==1.12.1\n",
      "anaconda-cloud-auth==0.1.3\n",
      "anaconda-navigator==2.5.2\n",
      "anaconda-project==0.11.1\n",
      "anyio==3.5.0\n",
      "appdirs==1.4.4\n",
      "argon2-cffi==21.3.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "arrow==1.2.3\n",
      "astroid==2.14.2\n",
      "astropy==5.1\n",
      "asttokens==2.0.5\n",
      "astunparse==1.6.3\n",
      "async-timeout==4.0.2\n",
      "atomicwrites==1.4.0\n",
      "attrs==22.1.0\n",
      "autopep8==1.6.0\n",
      "backcall==0.2.0\n",
      "backports.functools-lru-cache==1.6.4\n",
      "backports.tempfile==1.0\n",
      "backports.weakref==1.0.post1\n",
      "bcrypt==3.2.0\n",
      "beautifulsoup4==4.12.2\n",
      "binaryornot==0.4.4\n",
      "black==0.0\n",
      "bleach==4.1.0\n",
      "blosc2==2.0.0\n",
      "bokeh==3.2.1\n",
      "boltons==23.0.0\n",
      "botocore==1.29.76\n",
      "brotlipy==0.7.0\n",
      "cachetools==5.4.0\n",
      "certifi==2023.11.17\n",
      "cffi==1.15.1\n",
      "chardet==4.0.0\n",
      "charset-normalizer==2.0.4\n",
      "click==8.0.4\n",
      "cloudpickle==2.2.1\n",
      "clyent==1.2.2\n",
      "colorama==0.4.6\n",
      "colorcet==3.0.1\n",
      "comm==0.1.2\n",
      "conda==23.7.4\n",
      "conda-build==3.26.1\n",
      "conda-content-trust==0.2.0\n",
      "conda-index==0.3.0\n",
      "conda-libmamba-solver==23.7.0\n",
      "conda-pack==0.6.0\n",
      "conda-package-handling==2.2.0\n",
      "conda-package-streaming==0.9.0\n",
      "conda-repo-cli==1.0.75\n",
      "conda-token==0.4.0\n",
      "conda-verify==3.4.2\n",
      "constantly==15.1.0\n",
      "contourpy==1.0.5\n",
      "cookiecutter==1.7.3\n",
      "cryptography==41.0.3\n",
      "cssselect==1.1.0\n",
      "cycler==0.11.0\n",
      "cytoolz==0.12.0\n",
      "daal4py==2023.1.1\n",
      "dash==2.17.1\n",
      "dash-core-components==2.0.0\n",
      "dash-html-components==2.0.0\n",
      "dash-table==5.0.0\n",
      "dask==2023.6.0\n",
      "datasets==2.12.0\n",
      "datashader==0.15.2\n",
      "datashape==0.5.4\n",
      "debugpy==1.6.7\n",
      "decorator==5.1.1\n",
      "defusedxml==0.7.1\n",
      "diff-match-patch==20200713\n",
      "dill==0.3.6\n",
      "distributed==2023.6.0\n",
      "docstring-to-markdown==0.11\n",
      "docutils==0.18.1\n",
      "entrypoints==0.4\n",
      "et-xmlfile==1.1.0\n",
      "executing==0.8.3\n",
      "fastjsonschema==2.16.2\n",
      "filelock==3.9.0\n",
      "flake8==6.0.0\n",
      "flatbuffers==24.3.25\n",
      "fonttools==4.25.0\n",
      "frozenlist==1.3.3\n",
      "fsspec==2023.4.0\n",
      "future==0.18.3\n",
      "gast==0.4.0\n",
      "gensim==4.3.0\n",
      "glob2==0.7\n",
      "google-auth==2.32.0\n",
      "google-auth-oauthlib==1.0.0\n",
      "google-pasta==0.2.0\n",
      "greenlet==2.0.1\n",
      "grpcio==1.65.1\n",
      "h5py==3.11.0\n",
      "holoviews==1.17.1\n",
      "huggingface-hub==0.15.1\n",
      "hvplot==0.8.4\n",
      "hyperlink==21.0.0\n",
      "idna==3.4\n",
      "imagecodecs==2023.1.23\n",
      "imageio==2.26.0\n",
      "imagesize==1.4.1\n",
      "imbalanced-learn==0.10.1\n",
      "importlib-metadata==6.0.0\n",
      "incremental==21.3.0\n",
      "inflection==0.5.1\n",
      "iniconfig==1.1.1\n",
      "intake==0.6.8\n",
      "intervaltree==3.1.0\n",
      "ipykernel==6.25.0\n",
      "ipython==8.15.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==8.0.4\n",
      "isort==5.9.3\n",
      "itemadapter==0.3.0\n",
      "itemloaders==1.0.4\n",
      "itsdangerous==2.0.1\n",
      "jaraco.classes==3.2.1\n",
      "jedi==0.18.1\n",
      "jellyfish==1.0.1\n",
      "jinja2-time==0.2.0\n",
      "jmespath==0.10.0\n",
      "joblib==1.2.0\n",
      "json5==0.9.6\n",
      "jsonpatch==1.32\n",
      "jsonpointer==2.1\n",
      "jsonschema==4.17.3\n",
      "jupyter==1.0.0\n",
      "jupyter-client==7.4.9\n",
      "jupyter-console==6.6.3\n",
      "jupyter-core==5.3.0\n",
      "jupyter-events==0.6.3\n",
      "jupyter-server==1.23.4\n",
      "jupyter-server-fileid==0.9.0\n",
      "jupyter-server-ydoc==0.8.0\n",
      "jupyter-ydoc==0.2.4\n",
      "jupyterlab==3.6.3\n",
      "jupyterlab-pygments==0.1.2\n",
      "jupyterlab-server==2.22.0\n",
      "jupyterlab-widgets==3.0.5\n",
      "kaleido==0.2.1\n",
      "keras==3.4.1\n",
      "keyring==23.13.1\n",
      "kiwisolver==1.4.4\n",
      "lazy-loader==0.2\n",
      "lazy-object-proxy==1.6.0\n",
      "libarchive-c==2.9\n",
      "libclang==18.1.1\n",
      "libmambapy==1.5.1\n",
      "linkify-it-py==2.0.0\n",
      "llvmlite==0.40.0\n",
      "lmdb==1.4.1\n",
      "locket==1.0.0\n",
      "lxml==4.9.3\n",
      "lz4==4.3.2\n",
      "markdown-it-py==2.2.0\n",
      "matplotlib==3.7.2\n",
      "matplotlib-inline==0.1.6\n",
      "mccabe==0.7.0\n",
      "mdit-py-plugins==0.3.0\n",
      "mdurl==0.1.0\n",
      "menuinst==1.4.19\n",
      "mistune==0.8.4\n",
      "mkl-fft==1.3.8\n",
      "mkl-random==1.2.4\n",
      "mkl-service==2.4.0\n",
      "ml-dtypes==0.4.0\n",
      "more-itertools==8.12.0\n",
      "mpmath==1.3.0\n",
      "msgpack==1.0.3\n",
      "multidict==6.0.2\n",
      "multipledispatch==0.6.0\n",
      "multiprocess==0.70.14\n",
      "munkres==1.1.4\n",
      "mypy-extensions==1.0.0\n",
      "namex==0.0.8\n",
      "navigator-updater==0.4.0\n",
      "nbclassic==0.5.5\n",
      "nbclient==0.5.13\n",
      "nbconvert==6.5.4\n",
      "nbformat==5.9.2\n",
      "nest-asyncio==1.5.6\n",
      "networkx==3.1\n",
      "nltk==3.8.1\n",
      "notebook==6.5.4\n",
      "notebook-shim==0.2.2\n",
      "numba==0.57.1\n",
      "numexpr==2.8.4\n",
      "numpy==1.24.3\n",
      "numpydoc==1.5.0\n",
      "oauthlib==3.2.2\n",
      "open3d==0.18.0\n",
      "openpyxl==3.0.10\n",
      "opt-einsum==3.3.0\n",
      "optree==0.12.1\n",
      "packaging==23.1\n",
      "pandas==2.0.3\n",
      "pandocfilters==1.5.0\n",
      "panel==1.2.3\n",
      "param==1.13.0\n",
      "paramiko==2.8.1\n",
      "parsel==1.6.0\n",
      "parso==0.8.3\n",
      "partd==1.4.0\n",
      "pathlib==1.0.1\n",
      "pathspec==0.10.3\n",
      "patsy==0.5.3\n",
      "pep8==1.7.1\n",
      "pexpect==4.8.0\n",
      "pickleshare==0.7.5\n",
      "pip==23.2.1\n",
      "pkce==1.0.3\n",
      "pkginfo==1.9.6\n",
      "platformdirs==3.10.0\n",
      "plotly==5.9.0\n",
      "pluggy==1.0.0\n",
      "ply==3.11\n",
      "poyo==0.5.0\n",
      "prometheus-client==0.14.1\n",
      "prompt-toolkit==3.0.36\n",
      "protobuf==4.25.3\n",
      "psutil==5.9.0\n",
      "ptyprocess==0.7.0\n",
      "pure-eval==0.2.2\n",
      "pyOpenSSL==23.2.0\n",
      "py-cpuinfo==8.0.0\n",
      "pyarrow==11.0.0\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "pycodestyle==2.10.0\n",
      "pycosat==0.6.4\n",
      "pycparser==2.21\n",
      "pyct==0.5.0\n",
      "pycurl==7.45.2\n",
      "pydantic==1.10.8\n",
      "pydocstyle==6.3.0\n",
      "pyerfa==2.0.0\n",
      "pyflakes==3.0.1\n",
      "pylint==2.16.2\n",
      "pylint-venv==2.3.0\n",
      "pyls-spyder==0.4.0\n",
      "pyodbc==4.0.34\n",
      "pyparsing==3.0.9\n",
      "pyrsistent==0.18.0\n",
      "pytest==7.4.0\n",
      "python-dateutil==2.8.2\n",
      "python-dotenv==0.21.0\n",
      "python-json-logger==2.0.7\n",
      "python-lsp-black==1.2.1\n",
      "python-lsp-jsonrpc==1.0.0\n",
      "python-lsp-server==1.7.2\n",
      "python-slugify==5.0.2\n",
      "python-snappy==0.6.1\n",
      "pytoolconfig==1.2.5\n",
      "pytz==2023.3.post1\n",
      "pyviz-comms==2.3.0\n",
      "pywin32==305.1\n",
      "pywin32-ctypes==0.2.0\n",
      "pywinpty==2.0.10\n",
      "pyzmq==23.2.0\n",
      "qstylizer==0.2.2\n",
      "qtconsole==5.4.2\n",
      "queuelib==1.5.0\n",
      "regex==2022.7.9\n",
      "requests==2.31.0\n",
      "requests-file==1.5.1\n",
      "requests-oauthlib==2.0.0\n",
      "requests-toolbelt==1.0.0\n",
      "responses==0.13.3\n",
      "retrying==1.3.4\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "rich==13.7.1\n",
      "rope==1.7.0\n",
      "rsa==4.9\n",
      "ruamel.yaml==0.17.21\n",
      "ruamel-yaml-conda==0.17.21\n",
      "s3fs==2023.4.0\n",
      "safetensors==0.3.2\n",
      "scikit-image==0.20.0\n",
      "scikit-learn==1.3.0\n",
      "scikit-learn-intelex==20230426.121932\n",
      "scipy==1.11.1\n",
      "seaborn==0.12.2\n",
      "service-identity==18.1.0\n",
      "setuptools==69.0.3\n",
      "sip==6.6.2\n",
      "six==1.16.0\n",
      "smart-open==5.2.1\n",
      "sniffio==1.2.0\n",
      "snowballstemmer==2.2.0\n",
      "sortedcontainers==2.4.0\n",
      "soupsieve==2.4\n",
      "sphinxcontrib-applehelp==1.0.2\n",
      "sphinxcontrib-devhelp==1.0.2\n",
      "sphinxcontrib-htmlhelp==2.0.0\n",
      "sphinxcontrib-jsmath==1.0.1\n",
      "sphinxcontrib-qthelp==1.0.3\n",
      "sphinxcontrib-serializinghtml==1.1.5\n",
      "spyder==5.4.3\n",
      "spyder-kernels==2.4.4\n",
      "stack-data==0.2.0\n",
      "statsmodels==0.14.0\n",
      "sympy==1.11.1\n",
      "tables==3.8.0\n",
      "tabulate==0.8.10\n",
      "tblib==1.7.0\n",
      "tenacity==8.2.2\n",
      "tensorboard==2.17.0\n",
      "tensorboard-data-server==0.7.2\n",
      "tensorflow==2.17.0\n",
      "tensorflow-addons==0.22.0\n",
      "tensorflow-estimator==2.13.0\n",
      "tensorflow-intel==2.17.0\n",
      "tensorflow-io-gcs-filesystem==0.31.0\n",
      "termcolor==2.4.0\n",
      "terminado==0.17.1\n",
      "text-unidecode==1.3\n",
      "textdistance==4.2.1\n",
      "threadpoolctl==2.2.0\n",
      "three-merge==0.1.1\n",
      "tifffile==2023.4.12\n",
      "tinycss2==1.2.1\n",
      "tldextract==3.2.0\n",
      "tokenizers==0.13.2\n",
      "toml==0.10.2\n",
      "tomlkit==0.11.1\n",
      "toolz==0.12.0\n",
      "torch==2.4.0\n",
      "torchaudio==2.4.0\n",
      "torchvision==0.19.0\n",
      "tornado==6.3.2\n",
      "tqdm==4.65.0\n",
      "traitlets==5.7.1\n",
      "transformers==4.32.1\n",
      "trimesh==4.4.3\n",
      "twisted-iocpsupport==1.0.2\n",
      "typeguard==2.13.3\n",
      "typing-extensions==4.5.0\n",
      "tzdata==2023.3\n",
      "uc-micro-py==1.0.1\n",
      "ujson==5.4.0\n",
      "urllib3==1.26.16\n",
      "w3lib==1.21.0\n",
      "watchdog==2.1.6\n",
      "wcwidth==0.2.5\n",
      "webencodings==0.5.1\n",
      "websocket-client==0.58.0\n",
      "whatthepatch==1.0.2\n",
      "wheel==0.42.0\n",
      "widgetsnbextension==4.0.5\n",
      "win-inet-pton==1.1.0\n",
      "wrapt==1.14.1\n",
      "xarray==2023.6.0\n",
      "xlwings==0.29.1\n",
      "xxhash==2.0.2\n",
      "xyzservices==2022.9.0\n",
      "y-py==0.5.9\n",
      "yapf==0.31.0\n",
      "yarl==1.8.1\n",
      "ypy-websocket==0.8.2\n",
      "zict==2.2.0\n",
      "zipp==3.11.0\n",
      "zope.interface==5.4.0\n",
      "zstandard==0.19.0\n",
      "-ensorflow-intel==2.13.0\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "installed_packages = pkg_resources.working_set\n",
    "for package in installed_packages:\n",
    "    print(f\"{package.project_name}=={package.version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c3de072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 版本: 2.17.0\n",
      "Keras 版本: 3.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "print(\"TensorFlow 版本:\", tf.__version__)\n",
    "print(\"Keras 版本:\", keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d08b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5131d49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义UNET模型\n",
    "def unet_3d(input_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = tf.keras.layers.Conv3D(64, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(inputs)\n",
    "    conv1 = tf.keras.layers.Conv3D(64, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv1)\n",
    "    pool1 = tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(conv1)\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv3D(128, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pool1)\n",
    "    conv2 = tf.keras.layers.Conv3D(128, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv2)\n",
    "    pool2 = tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(conv2)\n",
    "\n",
    "    conv3 = tf.keras.layers.Conv3D(256, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pool2)\n",
    "    conv3 = tf.keras.layers.Conv3D(256, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv3)\n",
    "    pool3 = tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(conv3)\n",
    "\n",
    "    # Bottleneck\n",
    "    conv4 = tf.keras.layers.Conv3D(512, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pool3)\n",
    "    conv4 = tf.keras.layers.Conv3D(512, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv4)\n",
    "\n",
    "    # Decoder\n",
    "    up5 = tf.keras.layers.Conv3DTranspose(256, 2, strides=(2, 2, 2), padding='same')(conv4)\n",
    "    concat5 = tf.keras.layers.concatenate([up5, conv3], axis=-1)\n",
    "    conv5 = tf.keras.layers.Conv3D(256, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(concat5)\n",
    "    conv5 = tf.keras.layers.Conv3D(256, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv5)\n",
    "\n",
    "    up6 = tf.keras.layers.Conv3DTranspose(128, 2, strides=(2, 2, 2), padding='same')(conv5)\n",
    "    concat6 = tf.keras.layers.concatenate([up6, conv2], axis=-1)\n",
    "    conv6 = tf.keras.layers.Conv3D(128, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(concat6)\n",
    "    conv6 = tf.keras.layers.Conv3D(128, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv6)\n",
    "\n",
    "    up7 = tf.keras.layers.Conv3DTranspose(64, 2, strides=(2, 2, 2), padding='same')(conv6)\n",
    "    concat7 = tf.keras.layers.concatenate([up7, conv1], axis=-1)\n",
    "    conv7 = tf.keras.layers.Conv3D(64, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(concat7)\n",
    "    conv7 = tf.keras.layers.Conv3D(64, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv7)\n",
    "\n",
    "    # 修改输出层，适应3个类别\n",
    "    outputs = tf.keras.layers.Conv3D(3, 1, activation='softmax')(conv7)  # 3 channels for three-class classification\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "'''\n",
    "定义各种损失函数\n",
    "'''\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "# Focal\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        # 确保 y_true 和 y_pred 都是 float32 类型\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # 计算交叉熵\n",
    "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
    "        # 计算 focal 权重\n",
    "        focal_weight = alpha * tf.math.pow(1 - y_pred, gamma)\n",
    "        # 计算 focal loss\n",
    "        loss = focal_weight * cross_entropy\n",
    "        return tf.reduce_mean(tf.reduce_sum(loss, axis=-1))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "\n",
    "# Tversky\n",
    "def tversky_loss(y_true, y_pred, alpha=0.7, beta=0.3, smooth=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3, 4])\n",
    "    false_neg = tf.reduce_sum(y_true * (1 - y_pred), axis=[1, 2, 3, 4])\n",
    "    false_pos = tf.reduce_sum((1 - y_true) * y_pred, axis=[1, 2, 3, 4])\n",
    "    \n",
    "    tversky = (intersection + smooth) / (intersection + alpha * false_neg + beta * false_pos + smooth)\n",
    "    \n",
    "    return tf.reduce_mean(1 - tversky)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "使用 TensorFlow 实现 Sobel 边缘检测\n",
    "'''\n",
    "def sobel_filter_3d():\n",
    "    # 创建 Sobel 滤波器，注意这里设置 input_depth 为 1\n",
    "    filters = {\n",
    "        'x': tf.constant([[[[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]],\n",
    "                           [[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]],\n",
    "                           [[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]]]], dtype=tf.float32),\n",
    "        'y': tf.constant([[[[[[-1, -2, -1], [0, 0, 0], [1, 2, 1]]],\n",
    "                           [[[-1, -2, -1], [0, 0, 0], [1, 2, 1]]],\n",
    "                           [[[-1, -2, -1], [0, 0, 0], [1, 2, 1]]]]]], dtype=tf.float32),\n",
    "        'z': tf.constant([[[[[[-1, -2, -1], [0, 0, 0], [1, 2, 1]]],\n",
    "                           [[[-1, -2, -1], [0, 0, 0], [1, 2, 1]]],\n",
    "                           [[[-1, -2, -1], [0, 0, 0], [1, 2, 1]]]]]], dtype=tf.float32)\n",
    "    }\n",
    "    \n",
    "    # 过滤器的形状应该为 (depth, height, width, in_channels, out_channels)\n",
    "    filters['x'] = tf.reshape(filters['x'], [3, 3, 3, 1, 1])\n",
    "    filters['y'] = tf.reshape(filters['y'], [3, 3, 3, 1, 1])\n",
    "    filters['z'] = tf.reshape(filters['z'], [3, 3, 3, 1, 1])\n",
    "\n",
    "    return filters\n",
    "\n",
    "def apply_sobel_filters(volume, filters):\n",
    "    volume = tf.expand_dims(volume, axis=-1)  # Ensure volume has channel dimension\n",
    "    \n",
    "    conv_x = tf.nn.conv3d(volume, filters['x'], strides=[1, 1, 1, 1, 1], padding='SAME')\n",
    "    conv_y = tf.nn.conv3d(volume, filters['y'], strides=[1, 1, 1, 1, 1], padding='SAME')\n",
    "    conv_z = tf.nn.conv3d(volume, filters['z'], strides=[1, 1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    edges = tf.sqrt(tf.square(conv_x) + tf.square(conv_y) + tf.square(conv_z))\n",
    "    return edges\n",
    "\n",
    "def compute_sobel_edges_3d(volume):\n",
    "    filters = sobel_filter_3d()\n",
    "    edges = apply_sobel_filters(volume, filters)\n",
    "    return edges\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Boundary Loss\n",
    "'''\n",
    "# 欧几里得边界损失函数\n",
    "@tf.function\n",
    "def euclidean_distance_loss(true_edges, pred_edges):\n",
    "    distance = tf.sqrt(tf.reduce_sum(tf.square(true_edges - pred_edges), axis=[1, 2, 3, 4]))\n",
    "    return tf.reduce_mean(distance)\n",
    "\n",
    "# 二进制交集边界损失函数\n",
    "@tf.function\n",
    "def binary_intersection_loss(true_edges, pred_edges):\n",
    "    intersection = tf.reduce_sum(tf.cast(true_edges > 0, tf.float32) * tf.cast(pred_edges > 0, tf.float32), axis=[1, 2, 3, 4])\n",
    "    return tf.reduce_mean(1 - intersection)\n",
    "\n",
    "\n",
    "# 组合损失函数\n",
    "@tf.function\n",
    "def combined_loss(y_true, y_pred):\n",
    "    \n",
    "    # 确保 y_true 和 y_pred 是浮点型\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    # 使用掩码来只关注标签1和标签2\n",
    "    true_mask = tf.logical_or(tf.equal(y_true, 1), tf.equal(y_true, 2))\n",
    "    pred_mask = tf.logical_or(tf.equal(y_pred, 1), tf.equal(y_pred, 2))\n",
    "    \n",
    "    # 将掩码应用于标签网格和预测网格\n",
    "    y_true_masked = tf.where(true_mask, y_true, tf.zeros_like(y_true))\n",
    "    y_pred_masked = tf.where(pred_mask, y_pred, tf.zeros_like(y_pred))\n",
    "    \n",
    "    # 计算真实边界和预测边界\n",
    "    true_edges = compute_sobel_edges_3d(y_true_masked)\n",
    "    pred_edges = compute_sobel_edges_3d(y_pred_masked)\n",
    "    \n",
    "    # 计算边界损失\n",
    "    boundary_loss_euclidean = euclidean_distance_loss(true_edges, pred_edges)\n",
    "    boundary_loss_intersection = binary_intersection_loss(true_edges, pred_edges)\n",
    "    \n",
    "    # 定义损失函数\n",
    "    focal_loss_func = focal_loss(gamma=2.0, alpha=0.25)  # Create focal loss function\n",
    "    focal = focal_loss_func(y_true, y_pred)  # Compute focal loss\n",
    "    tversky = tversky_loss(y_true, y_pred)\n",
    "    \n",
    "    # 为每个损失函数赋权\n",
    "    weight_focal = 1.0\n",
    "    weight_tversky = 1.0\n",
    "    weight_euclidean = 0.01\n",
    "    weight_intersection = 1.0\n",
    "    \n",
    "    # 求和\n",
    "    total_loss = (weight_focal * focal + \n",
    "                  weight_tversky * tversky + \n",
    "                  weight_euclidean * boundary_loss_euclidean +\n",
    "                  weight_intersection * boundary_loss_intersection)\n",
    "    \n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f40dc94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Error when deserializing class 'Conv3DTranspose' using config={'name': 'conv3d_transpose', 'trainable': True, 'dtype': 'float32', 'filters': 256, 'kernel_size': [2, 2, 2], 'strides': [2, 2, 2], 'padding': 'same', 'data_format': 'channels_last', 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None, 'output_padding': None}.\n\nException encountered: Unrecognized keyword arguments passed to Conv3DTranspose: {'groups': 1}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\keras\\src\\ops\\operation.py:234\u001b[0m, in \u001b[0;36mOperation.from_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\conv3d_transpose.py:120\u001b[0m, in \u001b[0;36mConv3DTranspose.__init__\u001b[1;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    103\u001b[0m     filters,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    119\u001b[0m ):\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    121\u001b[0m         rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m    122\u001b[0m         filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    123\u001b[0m         kernel_size\u001b[38;5;241m=\u001b[39mkernel_size,\n\u001b[0;32m    124\u001b[0m         strides\u001b[38;5;241m=\u001b[39mstrides,\n\u001b[0;32m    125\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m    126\u001b[0m         data_format\u001b[38;5;241m=\u001b[39mdata_format,\n\u001b[0;32m    127\u001b[0m         dilation_rate\u001b[38;5;241m=\u001b[39mdilation_rate,\n\u001b[0;32m    128\u001b[0m         activation\u001b[38;5;241m=\u001b[39mactivation,\n\u001b[0;32m    129\u001b[0m         use_bias\u001b[38;5;241m=\u001b[39muse_bias,\n\u001b[0;32m    130\u001b[0m         kernel_initializer\u001b[38;5;241m=\u001b[39mkernel_initializer,\n\u001b[0;32m    131\u001b[0m         bias_initializer\u001b[38;5;241m=\u001b[39mbias_initializer,\n\u001b[0;32m    132\u001b[0m         kernel_regularizer\u001b[38;5;241m=\u001b[39mkernel_regularizer,\n\u001b[0;32m    133\u001b[0m         bias_regularizer\u001b[38;5;241m=\u001b[39mbias_regularizer,\n\u001b[0;32m    134\u001b[0m         activity_regularizer\u001b[38;5;241m=\u001b[39mactivity_regularizer,\n\u001b[0;32m    135\u001b[0m         kernel_constraint\u001b[38;5;241m=\u001b[39mkernel_constraint,\n\u001b[0;32m    136\u001b[0m         bias_constraint\u001b[38;5;241m=\u001b[39mbias_constraint,\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    138\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv_transpose.py:94\u001b[0m, in \u001b[0;36mBaseConvTranspose.__init__\u001b[1;34m(self, rank, filters, kernel_size, strides, padding, output_padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     73\u001b[0m     rank,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     93\u001b[0m ):\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     95\u001b[0m         trainable\u001b[38;5;241m=\u001b[39mtrainable,\n\u001b[0;32m     96\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m     97\u001b[0m         activity_regularizer\u001b[38;5;241m=\u001b[39mactivity_regularizer,\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     99\u001b[0m     )\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m=\u001b[39m rank\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\layer.py:266\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[1;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized keyword arguments \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    268\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    269\u001b[0m     )\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Will be determined in `build_wrapper`\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized keyword arguments passed to Conv3DTranspose: {'groups': 1}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 加载模型，如果已经有模型可以不运行这个cell\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model_load_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJuly30语义分割模型/UNET_前四个损失函数_无数据增强.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(model_load_path)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:189\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    183\u001b[0m         filepath,\n\u001b[0;32m    184\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    186\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    187\u001b[0m     )\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[0;32m    190\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[0;32m    191\u001b[0m     )\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:133\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    130\u001b[0m model_config \u001b[38;5;241m=\u001b[39m json_utils\u001b[38;5;241m.\u001b[39mdecode(model_config)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m saving_options\u001b[38;5;241m.\u001b[39mkeras_option_scope(use_legacy_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 133\u001b[0m     model \u001b[38;5;241m=\u001b[39m saving_utils\u001b[38;5;241m.\u001b[39mmodel_from_config(\n\u001b[0;32m    134\u001b[0m         model_config, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects\n\u001b[0;32m    135\u001b[0m     )\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# set weights\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     load_weights_from_hdf5_group(f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m], model)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py:85\u001b[0m, in \u001b[0;36mmodel_from_config\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# TODO(nkovela): Swap find and replace args during Keras 3.0 release\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Replace keras refs with keras\u001b[39;00m\n\u001b[0;32m     83\u001b[0m config \u001b[38;5;241m=\u001b[39m _find_replace_nested_dict(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m serialization\u001b[38;5;241m.\u001b[39mdeserialize_keras_object(\n\u001b[0;32m     86\u001b[0m     config,\n\u001b[0;32m     87\u001b[0m     module_objects\u001b[38;5;241m=\u001b[39mMODULE_OBJECTS\u001b[38;5;241m.\u001b[39mALL_OBJECTS,\n\u001b[0;32m     88\u001b[0m     custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m     89\u001b[0m     printable_module_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     90\u001b[0m )\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\keras\\src\\legacy\\saving\\serialization.py:495\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    490\u001b[0m cls_config \u001b[38;5;241m=\u001b[39m _find_replace_nested_dict(\n\u001b[0;32m    491\u001b[0m     cls_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    492\u001b[0m )\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_objects\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m arg_spec\u001b[38;5;241m.\u001b[39margs:\n\u001b[1;32m--> 495\u001b[0m     deserialized_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_config(\n\u001b[0;32m    496\u001b[0m         cls_config,\n\u001b[0;32m    497\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    498\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mobject_registration\u001b[38;5;241m.\u001b[39mGLOBAL_CUSTOM_OBJECTS,\n\u001b[0;32m    499\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcustom_objects,\n\u001b[0;32m    500\u001b[0m         },\n\u001b[0;32m    501\u001b[0m     )\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m object_registration\u001b[38;5;241m.\u001b[39mCustomObjectScope(custom_objects):\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\keras\\src\\models\\model.py:521\u001b[0m, in \u001b[0;36mModel.from_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_functional_config \u001b[38;5;129;01mand\u001b[39;00m revivable_as_functional:\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;66;03m# Revive Functional model\u001b[39;00m\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;66;03m# (but not Functional subclasses with a custom __init__)\u001b[39;00m\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional_from_config\n\u001b[1;32m--> 521\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m functional_from_config(\n\u001b[0;32m    522\u001b[0m         \u001b[38;5;28mcls\u001b[39m, config, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects\n\u001b[0;32m    523\u001b[0m     )\n\u001b[0;32m    525\u001b[0m \u001b[38;5;66;03m# Either the model has a custom __init__, or the config\u001b[39;00m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;66;03m# does not contain all the information necessary to\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;66;03m# revive a Functional model. This happens when the user creates\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;66;03m# In this case, we fall back to provide all config into the\u001b[39;00m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;66;03m# constructor of the class.\u001b[39;00m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\keras\\src\\models\\functional.py:477\u001b[0m, in \u001b[0;36mfunctional_from_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;66;03m# First, we create all layers and enqueue nodes to be processed\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_data \u001b[38;5;129;01min\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayers\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 477\u001b[0m     process_layer(layer_data)\n\u001b[0;32m    479\u001b[0m \u001b[38;5;66;03m# Then we process nodes in order of layer depth.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# Nodes that cannot yet be processed (if the inbound node\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# does not yet exist) are re-enqueued, and the process\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;66;03m# is repeated until all nodes are processed.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m unprocessed_nodes:\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\keras\\src\\models\\functional.py:457\u001b[0m, in \u001b[0;36mfunctional_from_config.<locals>.process_layer\u001b[1;34m(layer_data)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# Instantiate layer.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m layer_data:\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;66;03m# Legacy format deserialization (no \"module\" key)\u001b[39;00m\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;66;03m# used for H5 and SavedModel formats\u001b[39;00m\n\u001b[1;32m--> 457\u001b[0m     layer \u001b[38;5;241m=\u001b[39m saving_utils\u001b[38;5;241m.\u001b[39mmodel_from_config(\n\u001b[0;32m    458\u001b[0m         layer_data, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects\n\u001b[0;32m    459\u001b[0m     )\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     layer \u001b[38;5;241m=\u001b[39m serialization_lib\u001b[38;5;241m.\u001b[39mdeserialize_keras_object(\n\u001b[0;32m    462\u001b[0m         layer_data, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects\n\u001b[0;32m    463\u001b[0m     )\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py:85\u001b[0m, in \u001b[0;36mmodel_from_config\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# TODO(nkovela): Swap find and replace args during Keras 3.0 release\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Replace keras refs with keras\u001b[39;00m\n\u001b[0;32m     83\u001b[0m config \u001b[38;5;241m=\u001b[39m _find_replace_nested_dict(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m serialization\u001b[38;5;241m.\u001b[39mdeserialize_keras_object(\n\u001b[0;32m     86\u001b[0m     config,\n\u001b[0;32m     87\u001b[0m     module_objects\u001b[38;5;241m=\u001b[39mMODULE_OBJECTS\u001b[38;5;241m.\u001b[39mALL_OBJECTS,\n\u001b[0;32m     88\u001b[0m     custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m     89\u001b[0m     printable_module_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     90\u001b[0m )\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\keras\\src\\legacy\\saving\\serialization.py:504\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m object_registration\u001b[38;5;241m.\u001b[39mCustomObjectScope(custom_objects):\n\u001b[1;32m--> 504\u001b[0m             deserialized_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_config(cls_config)\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# Then `cls` may be a function returning a class.\u001b[39;00m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;66;03m# in this case by convention `config` holds\u001b[39;00m\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;66;03m# the kwargs of the function.\u001b[39;00m\n\u001b[0;32m    509\u001b[0m     custom_objects \u001b[38;5;241m=\u001b[39m custom_objects \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\keras\\src\\ops\\operation.py:236\u001b[0m, in \u001b[0;36mOperation.from_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when deserializing class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Error when deserializing class 'Conv3DTranspose' using config={'name': 'conv3d_transpose', 'trainable': True, 'dtype': 'float32', 'filters': 256, 'kernel_size': [2, 2, 2], 'strides': [2, 2, 2], 'padding': 'same', 'data_format': 'channels_last', 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None, 'output_padding': None}.\n\nException encountered: Unrecognized keyword arguments passed to Conv3DTranspose: {'groups': 1}"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 加载模型，如果已经有模型可以不运行这个cell\n",
    "model_load_path = r'July30语义分割模型/UNET_前四个损失函数_无数据增强.h5'\n",
    "model = tf.keras.models.load_model(model_load_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34883973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开启交互旋转\n",
    "%matplotlib notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f109a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "读取点云并对预测每点的类别标签\n",
    "'''\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "def load_obj_file(file_path):\n",
    "    vertices = []\n",
    "    faces = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('v '):\n",
    "                vertices.append(list(map(float, line.strip().split()[1:])))\n",
    "            elif line.startswith('f '):\n",
    "                faces.append([int(idx.split('/')[0]) - 1 for idx in line.strip().split()[1:]])\n",
    "    return np.array(vertices), np.array(faces)\n",
    "\n",
    "def center_vertices(vertices):\n",
    "    centroid = np.mean(vertices, axis=0)\n",
    "    return vertices - centroid\n",
    "\n",
    "def create_voxel_grid(points, grid_size):\n",
    "    min_coords = np.min(points, axis=0)\n",
    "    max_coords = np.max(points, axis=0)\n",
    "    voxel_dim = (max_coords - min_coords) / grid_size\n",
    "    \n",
    "    voxel_grid = np.zeros((grid_size, grid_size, grid_size), dtype=np.int32)\n",
    "    for point in points:\n",
    "        voxel = ((point - min_coords) / voxel_dim).astype(int)\n",
    "        voxel = np.clip(voxel, 0, grid_size-1)\n",
    "        voxel_grid[voxel[0], voxel[1], voxel[2]] += 1\n",
    "    \n",
    "    return voxel_grid, min_coords, voxel_dim\n",
    "\n",
    "def get_labels_from_model(model, voxel_grid):\n",
    "    voxel_grid = np.expand_dims(voxel_grid, axis=0)  # Add batch dimension\n",
    "    voxel_grid = np.expand_dims(voxel_grid, axis=-1)  # Add channel dimension\n",
    "    predictions = model.predict(voxel_grid)\n",
    "    \n",
    "    labels = np.argmax(predictions, axis=-1)  # Get the class with the highest probability\n",
    "    labels = labels.reshape(voxel_grid.shape[1], voxel_grid.shape[2], voxel_grid.shape[3])\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def apply_labels_to_point_cloud(data, predicted_labels, min_coords, voxel_dim, grid_size):\n",
    "    labels = np.zeros(len(data))\n",
    "    for i, point in enumerate(data):\n",
    "        voxel = ((point - min_coords) / voxel_dim).astype(int)\n",
    "        voxel = np.clip(voxel, 0, grid_size-1)\n",
    "        labels[i] = predicted_labels[voxel[0], voxel[1], voxel[2]]\n",
    "    return labels\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e6ef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import dcrf\n",
    "from dcrf import create_pairwise_gaussian, create_pairwise_bilateral, unary_from_softmax\n",
    "\n",
    "def load_obj_file(file_path):\n",
    "    vertices = []\n",
    "    faces = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('v '):\n",
    "                vertices.append(list(map(float, line.strip().split()[1:])))\n",
    "            elif line.startswith('f '):\n",
    "                faces.append([int(idx.split('/')[0]) - 1 for idx in line.strip().split()[1:]])\n",
    "    return np.array(vertices), np.array(faces)\n",
    "\n",
    "def center_vertices(vertices):\n",
    "    centroid = np.mean(vertices, axis=0)\n",
    "    return vertices - centroid\n",
    "\n",
    "def create_voxel_grid(points, grid_size):\n",
    "    min_coords = np.min(points, axis=0)\n",
    "    max_coords = np.max(points, axis=0)\n",
    "    voxel_dim = (max_coords - min_coords) / grid_size\n",
    "    \n",
    "    voxel_grid = np.zeros((grid_size, grid_size, grid_size), dtype=np.int32)\n",
    "    for point in points:\n",
    "        voxel = ((point - min_coords) / voxel_dim).astype(int)\n",
    "        voxel = np.clip(voxel, 0, grid_size-1)\n",
    "        voxel_grid[voxel[0], voxel[1], voxel[2]] += 1\n",
    "    \n",
    "    return voxel_grid, min_coords, voxel_dim\n",
    "\n",
    "def get_labels_from_model(model, voxel_grid):\n",
    "    voxel_grid = np.expand_dims(voxel_grid, axis=0)  # Add batch dimension\n",
    "    voxel_grid = np.expand_dims(voxel_grid, axis=-1)  # Add channel dimension\n",
    "    predictions = model.predict(voxel_grid)\n",
    "    \n",
    "    labels = np.argmax(predictions, axis=-1)  # Get the class with the highest probability\n",
    "    labels = labels.reshape(voxel_grid.shape[1], voxel_grid.shape[2], voxel_grid.shape[3])\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def upscale_voxel_grid(voxel_grid, new_grid_size):\n",
    "    old_size = voxel_grid.shape[0]\n",
    "    scale_factor = new_grid_size / old_size\n",
    "    \n",
    "    # Create an empty grid with new dimensions\n",
    "    new_voxel_grid = np.zeros((new_grid_size, new_grid_size, new_grid_size), dtype=np.int32)\n",
    "    \n",
    "    # Use interpolation to fill the new grid\n",
    "    for i in range(new_grid_size):\n",
    "        for j in range(new_grid_size):\n",
    "            for k in range(new_grid_size):\n",
    "                # Compute the corresponding coordinates in the old grid\n",
    "                old_i = int(i / scale_factor)\n",
    "                old_j = int(j / scale_factor)\n",
    "                old_k = int(k / scale_factor)\n",
    "                \n",
    "                # Use nearest neighbor interpolation for simplicity\n",
    "                new_voxel_grid[i, j, k] = voxel_grid[old_i, old_j, old_k]\n",
    "    \n",
    "    return new_voxel_grid\n",
    "\n",
    "def apply_crf(voxel_grid, probs):\n",
    "    grid_size = voxel_grid.shape[0]\n",
    "    num_voxels = grid_size * grid_size * grid_size\n",
    "\n",
    "    # 创建 DenseCRF 对象\n",
    "    d = dcrf.DenseCRF(num_voxels, 2)\n",
    "\n",
    "    # 创建 Unary 能量\n",
    "    probs = np.stack([1 - probs, probs], axis=0)  # Create the correct shape for probabilities\n",
    "    probs = np.ascontiguousarray(probs)\n",
    "    probs = probs.reshape((2, num_voxels))  # Reshape probabilities to (2, num_voxels)\n",
    "\n",
    "    U = unary_from_softmax(probs)\n",
    "    d.setUnaryEnergy(U)\n",
    "\n",
    "    # 增加高斯对势能（基于位置的光滑项）\n",
    "    pairwise_gaussian = create_pairwise_gaussian(sdims=(3, 3, 3), shape=(grid_size, grid_size, grid_size))\n",
    "    d.addPairwiseEnergy(pairwise_gaussian, compat=3)\n",
    "\n",
    "    # 增加双边对势能（基于颜色和位置的光滑项）\n",
    "    voxel_grid = voxel_grid.reshape((grid_size, grid_size, grid_size, 1))  # Reshape voxel grid to add a channel dimension\n",
    "    pairwise_bilateral = create_pairwise_bilateral(sdims=(3, 3, 3), schan=(1,), img=voxel_grid, chdim=3)\n",
    "    d.addPairwiseEnergy(pairwise_bilateral, compat=10)\n",
    "\n",
    "    # 进行推理\n",
    "    Q = d.inference(5)\n",
    "    Q = np.array(Q)  # 将 Q 转换为 NumPy 数组\n",
    "\n",
    "    # 获得分割结果\n",
    "    result = np.argmax(Q, axis=0).reshape((grid_size, grid_size, grid_size))\n",
    "\n",
    "    return result\n",
    "\n",
    "def apply_labels_to_point_cloud(data, predicted_labels, min_coords, voxel_dim, grid_size):\n",
    "    labels = np.zeros(len(data))\n",
    "    for i, point in enumerate(data):\n",
    "        voxel = ((point - min_coords) / voxel_dim).astype(int)\n",
    "        voxel = np.clip(voxel, 0, grid_size-1)\n",
    "        labels[i] = predicted_labels[voxel[0], voxel[1], voxel[2]]\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af4e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "给点云画拓扑图\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def build_topology_graph(vertices, k=10):\n",
    "    # 计算k近邻图\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(vertices)\n",
    "    distances, indices = nbrs.kneighbors(vertices)\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    for i, neighbors in enumerate(indices):\n",
    "        for j in neighbors:\n",
    "            if i != j:\n",
    "                G.add_edge(i, j)\n",
    "                \n",
    "    return G\n",
    "\n",
    "def largest_connected_component(G, nodes):\n",
    "    subgraph = G.subgraph(nodes)\n",
    "    largest_cc = max(nx.connected_components(subgraph), key=len)\n",
    "    return subgraph.subgraph(largest_cc)\n",
    "\n",
    "def get_max_connected_subgraphs(G, labels, target_labels):\n",
    "    subgraphs = {}\n",
    "    for label in target_labels:\n",
    "        nodes = [i for i, l in enumerate(labels) if l == label]\n",
    "        if nodes:\n",
    "            largest_cc = largest_connected_component(G, nodes)\n",
    "            subgraphs[label] = largest_cc\n",
    "    return subgraphs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe7599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "找1-2的边界线\n",
    "'''\n",
    "\n",
    "def find_boundary_edges(G, subgraph1, subgraph2):\n",
    "    boundary_edges = []\n",
    "    for edge in G.edges():\n",
    "        if (edge[0] in subgraph1 and edge[1] in subgraph2) or (edge[0] in subgraph2 and edge[1] in subgraph1):\n",
    "            boundary_edges.append(edge)\n",
    "    return boundary_edges\n",
    "\n",
    "def plot_surface_with_boundary_lines(vertices, faces, labels, boundary_edges, view_angles=(30, 30), angles=(0, 0, 0)):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    vertices = np.array(vertices)\n",
    "    faces = np.array(faces)\n",
    "    \n",
    "    # Apply rotation\n",
    "    vertices = rotate_points(vertices, angles)\n",
    "    \n",
    "    x, y, z = vertices.T\n",
    "    \n",
    "    # 绘制不同标签区域的点云\n",
    "    colors = ['lightgreen', 'cornflowerblue', 'honeydew']\n",
    "    for i in range(3):\n",
    "        part_faces = [face for face in faces if sum(labels[vertex] == i for vertex in face) > 1]\n",
    "        if len(part_faces) > 0:\n",
    "            ax.plot_trisurf(vertices[:, 0], vertices[:, 1], vertices[:, 2], triangles=part_faces, color=colors[i], alpha=0.6)\n",
    "    \n",
    "    # 绘制边界线\n",
    "    for edge in boundary_edges:\n",
    "        p1, p2 = vertices[list(edge)]\n",
    "        ax.plot([p1[0], p2[0]], [p1[1], p2[1]], [p1[2], p2[2]], color='red', lw=1)\n",
    "    \n",
    "    # 设置标签和标题\n",
    "    ax.set_xlabel('X axis')\n",
    "    ax.set_ylabel('Y axis')\n",
    "    ax.set_zlabel('Z axis')\n",
    "    ax.set_title('3D Model with Boundary Lines')\n",
    "\n",
    "    # 确保坐标轴刻度一致\n",
    "    max_range = np.array([max(x)-min(x), max(y)-min(y), max(z)-min(z)]).max()\n",
    "    mid_x = (max(x) + min(x)) * 0.5\n",
    "    mid_y = (max(y) + min(y)) * 0.5\n",
    "    mid_z = (max(z) + min(z)) * 0.5\n",
    "    ax.set_xlim(mid_x - max_range/2, mid_x + max_range/2)\n",
    "    ax.set_ylim(mid_y - max_range/2, mid_y + max_range/2)\n",
    "    ax.set_zlim(mid_z - max_range/2, mid_z + max_range/2)\n",
    "\n",
    "    # 设置视角\n",
    "    elev, azim = view_angles\n",
    "    ax.view_init(elev=elev, azim=azim)  # Adjust these values as needed\n",
    "\n",
    "    # 确保坐标轴比例相等\n",
    "    ax.set_box_aspect([1,1,1])  # Aspect ratio is 1:1:1\n",
    "\n",
    "    # 启用交互式旋转\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef17753",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "设置视角\n",
    "'''\n",
    "\n",
    "def rotate_points(points, angles):\n",
    "    x_angle, y_angle, z_angle = angles\n",
    "    Rx = np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, np.cos(x_angle), -np.sin(x_angle)],\n",
    "        [0, np.sin(x_angle), np.cos(x_angle)]\n",
    "    ])\n",
    "    Ry = np.array([\n",
    "        [np.cos(y_angle), 0, np.sin(y_angle)],\n",
    "        [0, 1, 0],\n",
    "        [-np.sin(y_angle), 0, np.cos(y_angle)]\n",
    "    ])\n",
    "    Rz = np.array([\n",
    "        [np.cos(z_angle), -np.sin(z_angle), 0],\n",
    "        [np.sin(z_angle), np.cos(z_angle), 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    R = Rz @ Ry @ Rx\n",
    "    return points @ R.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8882750",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "出图\n",
    "'''\n",
    "\n",
    "# 加载点云数据\n",
    "obj_file_path = r'D:/李娅宁/肩台外侧点-0715/已完成预处理的原始数据/15/15_1.obj'\n",
    "vertices, faces = load_obj_file(obj_file_path)\n",
    "centered_vertices = center_vertices(vertices)\n",
    "\n",
    "# 将点云转换为体素网格\n",
    "grid_size = 16  # Ensure grid size matches model requirements\n",
    "voxel_grid, min_coords, voxel_dim = create_voxel_grid(np.array(centered_vertices), grid_size)\n",
    "\n",
    "# 使用训练好的模型进行预测\n",
    "predicted_labels = get_labels_from_model(model, voxel_grid)\n",
    "\n",
    "# 等比扩大体素网格\n",
    "new_grid_size = 128  # Example new grid size, adjust as needed\n",
    "upscaled_voxel_grid = upscale_voxel_grid(voxel_grid, new_grid_size)\n",
    "\n",
    "# 使用CRF后处理\n",
    "probs = np.array(predicted_labels, dtype=np.float32) / np.max(predicted_labels)  # Normalize probabilities\n",
    "predicted_labels_crf = apply_crf(upscaled_voxel_grid, probs)\n",
    "\n",
    "# 获取原始点云的预测标签\n",
    "predicted_point_labels = apply_labels_to_point_cloud(np.array(centered_vertices), predicted_labels_crf, min_coords, voxel_dim, new_grid_size)\n",
    "\n",
    "# 构建拓扑图\n",
    "G = build_topology_graph(centered_vertices)\n",
    "\n",
    "# 找到最大连通子图\n",
    "target_labels = [1, 2]\n",
    "subgraphs = get_max_connected_subgraphs(G, predicted_point_labels, target_labels)\n",
    "\n",
    "# 找到边界线\n",
    "boundary_edges = find_boundary_edges(G, subgraphs[1], subgraphs[2])\n",
    "\n",
    "# 绘制点云及边界线\n",
    "plot_surface_with_boundary_lines(centered_vertices, faces, predicted_point_labels, boundary_edges, view_angles=(30, 30), angles=(np.radians(90), np.radians(-30), np.radians(30)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7af414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

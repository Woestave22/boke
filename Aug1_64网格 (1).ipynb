{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea6c7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "将体素网格的尺寸调大到64，以平滑边缘线\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457acbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "统一设置地址\n",
    "'''\n",
    "\n",
    "import os\n",
    "\n",
    "# 获取当前工作目录\n",
    "current_dir = os.getcwd()\n",
    "print(\"当前工作目录：\", current_dir)\n",
    "\n",
    "# 修改当前工作目录，以后输出文件只需要写文件名\n",
    "new_dir = \"D:/李娅宁/肩台外侧点-0715/\"\n",
    "os.chdir(new_dir)\n",
    "print(\"修改后的工作目录：\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18072197",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "训练UNET\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac74d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "加载并处理数据\n",
    "非常慢，建议每次加载并处理后先行保存\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate\n",
    "from scipy.stats import mode\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 读取点云数据和标签\n",
    "def load_labeled_point_cloud(file_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 4:\n",
    "                x, y, z, label = map(float, parts)\n",
    "                data.append([x, y, z])\n",
    "                labels.append(int(label))\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# 创建带标签的体素网格\n",
    "def create_labeled_voxel_grid(data, labels, grid_size):\n",
    "    if data.size == 0 or labels.size == 0:\n",
    "        print(f\"Warning: Empty data or labels array. data.size={data.size}, labels.size={labels.size}\")\n",
    "        return None, None\n",
    "\n",
    "    grid = np.zeros((grid_size, grid_size, grid_size))\n",
    "    label_grid = np.zeros((grid_size, grid_size, grid_size), dtype=int)  # Initialize with 0 to indicate empty space\n",
    "\n",
    "    min_coords = np.min(data, axis=0)\n",
    "    max_coords = np.max(data, axis=0)\n",
    "    voxel_dim = (max_coords - min_coords) / grid_size\n",
    "\n",
    "    voxel_label_dict = {}\n",
    "\n",
    "    for i, point in enumerate(data):\n",
    "        voxel = ((point - min_coords) / voxel_dim).astype(int)\n",
    "        voxel = np.clip(voxel, 0, grid_size-1)  # Ensure indices are within bounds\n",
    "        voxel_key = tuple(voxel)\n",
    "        \n",
    "        if voxel_key not in voxel_label_dict:\n",
    "            voxel_label_dict[voxel_key] = []\n",
    "        voxel_label_dict[voxel_key].append(labels[i])\n",
    "\n",
    "    for voxel_key, label_list in voxel_label_dict.items():\n",
    "        # 计算众数\n",
    "        mode_result = mode(label_list)\n",
    "        \n",
    "        # 获取众数标签\n",
    "        most_common_label = mode_result.mode  # mode_result.mode 是一个标量\n",
    "        \n",
    "        grid[voxel_key] = 1\n",
    "        label_grid[voxel_key] = most_common_label\n",
    "\n",
    "    return grid, label_grid\n",
    "\n",
    "# 数据增强，对体素网格进行随机三轴小幅旋转\n",
    "def augment_voxel_grid(voxel_grid):\n",
    "    # 随机旋转角度（-10到10度）\n",
    "    angles = np.random.uniform(-10, 10, size=3)\n",
    "    \n",
    "    # 对每个轴进行旋转\n",
    "    voxel_grid = rotate(voxel_grid, angle=angles[0], axes=(1, 0), reshape=False, order=1, mode='nearest')\n",
    "    voxel_grid = rotate(voxel_grid, angle=angles[1], axes=(2, 0), reshape=False, order=1, mode='nearest')\n",
    "    voxel_grid = rotate(voxel_grid, angle=angles[2], axes=(2, 1), reshape=False, order=1, mode='nearest')\n",
    "    \n",
    "    return voxel_grid\n",
    "\n",
    "\n",
    "# 从文件夹中读取数据并加工成体素网格\n",
    "def load_data_from_directory(data_dir, grid_size=64):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith(\"_labeled.txt\"):\n",
    "            file_path = os.path.join(data_dir, file)\n",
    "            data, labels = load_labeled_point_cloud(file_path)\n",
    "            if data.size == 0 or labels.size == 0:\n",
    "                print(f\"Skipping empty file: {file_path}\")\n",
    "                continue\n",
    "            voxel_grid, label_grid = create_labeled_voxel_grid(data, labels, grid_size)\n",
    "            if voxel_grid is not None and label_grid is not None:\n",
    "                # 关闭数据增强\n",
    "                # voxel_grid = augment_voxel_grid(voxel_grid)\n",
    "                x_data.append(voxel_grid)\n",
    "                y_data.append(label_grid)\n",
    "    x_data = np.expand_dims(np.array(x_data), axis=-1)\n",
    "    y_data = np.expand_dims(np.array(y_data), axis=-1)\n",
    "    return x_data, y_data\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "data_dir = '重新处理后的数据_手工筛选'\n",
    "x_data, y_data = load_data_from_directory(data_dir, grid_size=64)\n",
    "\n",
    "# 确保数据归一化\n",
    "x_data = x_data / np.max(x_data)\n",
    "\n",
    "# 确保标签是0, 1, 2\n",
    "y_data = y_data.astype(np.int32)\n",
    "\n",
    "# 划分数据集\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_data, y_data, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存加载好的数据备用\n",
    "def save_processed_data(x_data, y_data, folder_path, file_prefix):\n",
    "\n",
    "    # 确保文件夹存在\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # 保存数据到指定文件夹\n",
    "    np.save(os.path.join(folder_path, f'{file_prefix}_x_data.npy'), x_data)\n",
    "    np.save(os.path.join(folder_path, f'{file_prefix}_y_data.npy'), y_data)\n",
    "\n",
    "data_folder_path = 'Aug1语义分割模型'\n",
    "data_file_prefix = 'Aug1语义分割数据64'\n",
    "save_processed_data(x_data, y_data, data_folder_path, data_file_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440d606c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce698a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "重新加载已保存的处理好的数据\n",
    "如果已有数据则跳过此cell\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate\n",
    "from scipy.stats import mode\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_processed_data(folder_path, file_prefix):\n",
    "    x_data = np.load(os.path.join(folder_path, f'{file_prefix}_x_data.npy'))\n",
    "    y_data = np.load(os.path.join(folder_path, f'{file_prefix}_y_data.npy'))\n",
    "    return x_data, y_data\n",
    "\n",
    "# 加载保存的数据\n",
    "data_folder_path = 'Aug1语义分割模型'\n",
    "data_file_prefix = 'Aug1语义分割数据64'\n",
    "x_data, y_data = load_processed_data(data_folder_path, data_file_prefix)\n",
    "\n",
    "\n",
    "# 划分数据集\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_data, y_data, test_size=0.2, random_state=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5e0bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练过程中的损失和精确度\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    绘制训练过程中的损失和精确度。\n",
    "    \"\"\"\n",
    "    # 绘制损失\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # 绘制精确度\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a4ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义UNET模型\n",
    "def unet_3d(input_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = tf.keras.layers.Conv3D(64, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(inputs)\n",
    "    conv1 = tf.keras.layers.Conv3D(64, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv1)\n",
    "    pool1 = tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(conv1)\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv3D(128, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pool1)\n",
    "    conv2 = tf.keras.layers.Conv3D(128, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv2)\n",
    "    pool2 = tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(conv2)\n",
    "\n",
    "    conv3 = tf.keras.layers.Conv3D(256, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pool2)\n",
    "    conv3 = tf.keras.layers.Conv3D(256, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv3)\n",
    "    pool3 = tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(conv3)\n",
    "\n",
    "    # Bottleneck\n",
    "    conv4 = tf.keras.layers.Conv3D(512, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pool3)\n",
    "    conv4 = tf.keras.layers.Conv3D(512, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv4)\n",
    "\n",
    "    # Decoder\n",
    "    up5 = tf.keras.layers.Conv3DTranspose(256, 2, strides=(2, 2, 2), padding='same')(conv4)\n",
    "    concat5 = tf.keras.layers.concatenate([up5, conv3], axis=-1)\n",
    "    conv5 = tf.keras.layers.Conv3D(256, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(concat5)\n",
    "    conv5 = tf.keras.layers.Conv3D(256, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv5)\n",
    "\n",
    "    up6 = tf.keras.layers.Conv3DTranspose(128, 2, strides=(2, 2, 2), padding='same')(conv5)\n",
    "    concat6 = tf.keras.layers.concatenate([up6, conv2], axis=-1)\n",
    "    conv6 = tf.keras.layers.Conv3D(128, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(concat6)\n",
    "    conv6 = tf.keras.layers.Conv3D(128, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv6)\n",
    "\n",
    "    up7 = tf.keras.layers.Conv3DTranspose(64, 2, strides=(2, 2, 2), padding='same')(conv6)\n",
    "    concat7 = tf.keras.layers.concatenate([up7, conv1], axis=-1)\n",
    "    conv7 = tf.keras.layers.Conv3D(64, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(concat7)\n",
    "    conv7 = tf.keras.layers.Conv3D(64, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv7)\n",
    "\n",
    "    # 修改输出层，适应3个类别\n",
    "    outputs = tf.keras.layers.Conv3D(3, 1, activation='softmax')(conv7)  # 3 channels for three-class classification\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c56da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "定义各种损失函数\n",
    "'''\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "# 交叉熵\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True))\n",
    "\n",
    "# Dice\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3, 4])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2, 3, 4]) + tf.reduce_sum(y_pred, axis=[1, 2, 3, 4])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    return tf.reduce_mean(1 - dice)\n",
    "\n",
    "# Focal\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        # 确保 y_true 和 y_pred 都是 float32 类型\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # 计算交叉熵\n",
    "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
    "        # 计算 focal 权重\n",
    "        focal_weight = alpha * tf.math.pow(1 - y_pred, gamma)\n",
    "        # 计算 focal loss\n",
    "        loss = focal_weight * cross_entropy\n",
    "        return tf.reduce_mean(tf.reduce_sum(loss, axis=-1))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "\n",
    "# Tversky\n",
    "def tversky_loss(y_true, y_pred, alpha=0.7, beta=0.3, smooth=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3, 4])\n",
    "    false_neg = tf.reduce_sum(y_true * (1 - y_pred), axis=[1, 2, 3, 4])\n",
    "    false_pos = tf.reduce_sum((1 - y_true) * y_pred, axis=[1, 2, 3, 4])\n",
    "    \n",
    "    tversky = (intersection + smooth) / (intersection + alpha * false_neg + beta * false_pos + smooth)\n",
    "    \n",
    "    return tf.reduce_mean(1 - tversky)\n",
    "\n",
    "\n",
    "# 组合损失函数\n",
    "@tf.function\n",
    "def combined_loss(y_true, y_pred):\n",
    "    ce_loss = cross_entropy_loss(y_true, y_pred)\n",
    "    dice = dice_loss(y_true, y_pred)\n",
    "    tversky = tversky_loss(y_true, y_pred)\n",
    "    focal = focal_loss(gamma=2.0, alpha=0.25)(y_true, y_pred)\n",
    "    \n",
    "    return ce_loss + dice + tversky + focal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85edf2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "input_shape = (64, 64, 64, 1)\n",
    "model = unet_3d(input_shape)\n",
    "\n",
    "# 定义学习率调度函数\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return float(lr)\n",
    "    else:\n",
    "        return float(lr * tf.math.exp(-0.1).numpy())\n",
    "\n",
    "# 定义学习率调度回调和早停回调\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00003), \n",
    "              loss=combined_loss,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "history = model.fit(\n",
    "    x_train, y_train, \n",
    "    validation_split=0.1,\n",
    "    epochs=20, \n",
    "    batch_size=10,\n",
    "    callbacks=[lr_scheduler, early_stopping]\n",
    ")\n",
    "\n",
    "# 评估模型\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# 显示炼丹过程\n",
    "plot_training_history(history)\n",
    "\n",
    "# 保存模型\n",
    "model_save_path = 'Aug1语义分割模型/UNET64.h5'\n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb8800f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef244bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "预测及可视化\n",
    "基于拓扑图绘制1-2区域分界线\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b058ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型，如果已经有模型可以不运行这个cell\n",
    "model_load_path = 'Aug1语义分割模型/UNET64.h5'\n",
    "\n",
    "# 自定义损失函数字典\n",
    "custom_objects = {\n",
    "    'cross_entropy_loss': cross_entropy_loss,\n",
    "    'dice_loss': dice_loss,\n",
    "    'focal_loss': focal_loss,\n",
    "    'tversky_loss': tversky_loss,\n",
    "    'combined_loss': combined_loss\n",
    "}\n",
    "\n",
    "try:\n",
    "    model = tf.keras.models.load_model(model_load_path, custom_objects=custom_objects)\n",
    "    print(\"模型加载成功！\")\n",
    "except Exception as e:\n",
    "    print(\"模型加载失败:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ca070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开启交互旋转\n",
    "%matplotlib notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64832996",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "读取点云并对预测每点的类别标签\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "def load_obj_file(file_path):\n",
    "    vertices = []\n",
    "    faces = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('v '):\n",
    "                vertices.append(list(map(float, line.strip().split()[1:])))\n",
    "            elif line.startswith('f '):\n",
    "                faces.append([int(idx.split('/')[0]) - 1 for idx in line.strip().split()[1:]])\n",
    "    return np.array(vertices), np.array(faces)\n",
    "\n",
    "def center_vertices(vertices):\n",
    "    centroid = np.mean(vertices, axis=0)\n",
    "    return vertices - centroid\n",
    "\n",
    "def create_voxel_grid(points, grid_size):\n",
    "    min_coords = np.min(points, axis=0)\n",
    "    max_coords = np.max(points, axis=0)\n",
    "    voxel_dim = (max_coords - min_coords) / grid_size\n",
    "    \n",
    "    voxel_grid = np.zeros((grid_size, grid_size, grid_size), dtype=np.int32)\n",
    "    for point in points:\n",
    "        voxel = ((point - min_coords) / voxel_dim).astype(int)\n",
    "        voxel = np.clip(voxel, 0, grid_size-1)\n",
    "        voxel_grid[voxel[0], voxel[1], voxel[2]] += 1\n",
    "    \n",
    "    return voxel_grid, min_coords, voxel_dim\n",
    "\n",
    "def get_labels_from_model(model, voxel_grid):\n",
    "    voxel_grid = np.expand_dims(voxel_grid, axis=0)  # Add batch dimension\n",
    "    voxel_grid = np.expand_dims(voxel_grid, axis=-1)  # Add channel dimension\n",
    "    predictions = model.predict(voxel_grid)\n",
    "    \n",
    "    labels = np.argmax(predictions, axis=-1)  # Get the class with the highest probability\n",
    "    labels = labels.reshape(voxel_grid.shape[1], voxel_grid.shape[2], voxel_grid.shape[3])\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def apply_labels_to_point_cloud(data, predicted_labels, min_coords, voxel_dim, grid_size):\n",
    "    labels = np.zeros(len(data))\n",
    "    for i, point in enumerate(data):\n",
    "        voxel = ((point - min_coords) / voxel_dim).astype(int)\n",
    "        voxel = np.clip(voxel, 0, grid_size-1)\n",
    "        labels[i] = predicted_labels[voxel[0], voxel[1], voxel[2]]\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42e309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "给点云画拓扑图\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def build_topology_graph(vertices, k=10):\n",
    "    # 计算k近邻图\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(vertices)\n",
    "    distances, indices = nbrs.kneighbors(vertices)\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    for i, neighbors in enumerate(indices):\n",
    "        for j in neighbors:\n",
    "            if i != j:\n",
    "                G.add_edge(i, j)\n",
    "                \n",
    "    return G\n",
    "\n",
    "def largest_connected_component(G, nodes):\n",
    "    subgraph = G.subgraph(nodes)\n",
    "    largest_cc = max(nx.connected_components(subgraph), key=len)\n",
    "    return subgraph.subgraph(largest_cc)\n",
    "\n",
    "def get_max_connected_subgraphs(G, labels, target_labels):\n",
    "    subgraphs = {}\n",
    "    for label in target_labels:\n",
    "        nodes = [i for i, l in enumerate(labels) if l == label]\n",
    "        if nodes:\n",
    "            largest_cc = largest_connected_component(G, nodes)\n",
    "            subgraphs[label] = largest_cc\n",
    "    return subgraphs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c93a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "找1-2的边界线\n",
    "'''\n",
    "\n",
    "def find_boundary_edges(G, subgraph1, subgraph2):\n",
    "    boundary_edges = []\n",
    "    for edge in G.edges():\n",
    "        if (edge[0] in subgraph1 and edge[1] in subgraph2) or (edge[0] in subgraph2 and edge[1] in subgraph1):\n",
    "            boundary_edges.append(edge)\n",
    "    return boundary_edges\n",
    "\n",
    "def plot_surface_with_boundary_lines(vertices, faces, labels, boundary_edges, view_angles=(30, 30), angles=(0, 0, 0)):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    vertices = np.array(vertices)\n",
    "    faces = np.array(faces)\n",
    "    \n",
    "    # Apply rotation\n",
    "    vertices = rotate_points(vertices, angles)\n",
    "    \n",
    "    x, y, z = vertices.T\n",
    "    \n",
    "    # 绘制不同标签区域的点云\n",
    "    colors = ['lightgreen', 'cornflowerblue', 'honeydew']\n",
    "    for i in range(3):\n",
    "        part_faces = [face for face in faces if sum(labels[vertex] == i for vertex in face) > 1]\n",
    "        if len(part_faces) > 0:\n",
    "            ax.plot_trisurf(vertices[:, 0], vertices[:, 1], vertices[:, 2], triangles=part_faces, color=colors[i], alpha=0.6)\n",
    "    \n",
    "    # 绘制边界线\n",
    "    for edge in boundary_edges:\n",
    "        p1, p2 = vertices[list(edge)]\n",
    "        ax.plot([p1[0], p2[0]], [p1[1], p2[1]], [p1[2], p2[2]], color='red', lw=1)\n",
    "    \n",
    "    # 设置标签和标题\n",
    "    ax.set_xlabel('X axis')\n",
    "    ax.set_ylabel('Y axis')\n",
    "    ax.set_zlabel('Z axis')\n",
    "    ax.set_title('3D Model with Boundary Lines')\n",
    "\n",
    "    # 确保坐标轴刻度一致\n",
    "    max_range = np.array([max(x)-min(x), max(y)-min(y), max(z)-min(z)]).max()\n",
    "    mid_x = (max(x) + min(x)) * 0.5\n",
    "    mid_y = (max(y) + min(y)) * 0.5\n",
    "    mid_z = (max(z) + min(z)) * 0.5\n",
    "    ax.set_xlim(mid_x - max_range/2, mid_x + max_range/2)\n",
    "    ax.set_ylim(mid_y - max_range/2, mid_y + max_range/2)\n",
    "    ax.set_zlim(mid_z - max_range/2, mid_z + max_range/2)\n",
    "\n",
    "    # 设置视角\n",
    "    elev, azim = view_angles\n",
    "    ax.view_init(elev=elev, azim=azim)  # Adjust these values as needed\n",
    "\n",
    "    # 确保坐标轴比例相等\n",
    "    ax.set_box_aspect([1,1,1])  # Aspect ratio is 1:1:1\n",
    "\n",
    "    # 启用交互式旋转\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0af511",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "设置视角\n",
    "'''\n",
    "\n",
    "def rotate_points(points, angles):\n",
    "    x_angle, y_angle, z_angle = angles\n",
    "    Rx = np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, np.cos(x_angle), -np.sin(x_angle)],\n",
    "        [0, np.sin(x_angle), np.cos(x_angle)]\n",
    "    ])\n",
    "    Ry = np.array([\n",
    "        [np.cos(y_angle), 0, np.sin(y_angle)],\n",
    "        [0, 1, 0],\n",
    "        [-np.sin(y_angle), 0, np.cos(y_angle)]\n",
    "    ])\n",
    "    Rz = np.array([\n",
    "        [np.cos(z_angle), -np.sin(z_angle), 0],\n",
    "        [np.sin(z_angle), np.cos(z_angle), 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    R = Rz @ Ry @ Rx\n",
    "    return points @ R.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99fbbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "出图\n",
    "'''\n",
    "\n",
    "# 加载点云数据\n",
    "obj_file_path = r'D:/李娅宁/肩台外侧点-0715/已完成预处理的原始数据/15/15_1.obj'\n",
    "vertices, faces = load_obj_file(obj_file_path)\n",
    "centered_vertices = center_vertices(vertices)\n",
    "\n",
    "# 将点云转换为体素网格\n",
    "grid_size = 64  # Ensure grid size matches model requirements\n",
    "voxel_grid, min_coords, voxel_dim = create_voxel_grid(np.array(centered_vertices), grid_size)\n",
    "\n",
    "# 使用训练好的模型进行预测\n",
    "predicted_labels = get_labels_from_model(model, voxel_grid)\n",
    "\n",
    "# 获取原始点云的预测标签\n",
    "predicted_point_labels = apply_labels_to_point_cloud(np.array(centered_vertices), predicted_labels, min_coords, voxel_dim, grid_size)\n",
    "\n",
    "# 构建拓扑图\n",
    "G = build_topology_graph(centered_vertices)\n",
    "\n",
    "# 找到最大连通子图\n",
    "target_labels = [1, 2]\n",
    "subgraphs = get_max_connected_subgraphs(G, predicted_point_labels, target_labels)\n",
    "\n",
    "# 找到边界线\n",
    "boundary_edges = find_boundary_edges(G, subgraphs[1], subgraphs[2])\n",
    "\n",
    "# 绘制点云及边界线\n",
    "plot_surface_with_boundary_lines(centered_vertices, faces, predicted_point_labels, boundary_edges, view_angles=(30, 30), angles=(np.radians(90), np.radians(-30), np.radians(30)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pydensecrf-env)",
   "language": "python",
   "name": "pydensecrf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

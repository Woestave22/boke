{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39256826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n当前模型无法输出清晰的分界线，这里枚举了一些针对性措施\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "当前模型无法输出清晰的分界线，这里枚举了一些针对性措施\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3ad2a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录： C:\\Users\\HP\n",
      "修改后的工作目录： D:\\李娅宁\\肩台外侧点-0715\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "统一设置地址\n",
    "'''\n",
    "\n",
    "import os\n",
    "\n",
    "# 获取当前工作目录\n",
    "current_dir = os.getcwd()\n",
    "print(\"当前工作目录：\", current_dir)\n",
    "\n",
    "# 修改当前工作目录，以后输出文件只需要写文件名\n",
    "new_dir = \"D:/李娅宁/肩台外侧点-0715/\"\n",
    "os.chdir(new_dir)\n",
    "print(\"修改后的工作目录：\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f1ec474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsobel算子滤波\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "sobel算子滤波\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d323de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (None, 16, 16, 16, 1)\n",
      "conv1 shape: (None, 16, 16, 16, 64)\n",
      "conv1 shape: (None, 16, 16, 16, 64)\n",
      "pool1 shape: (None, 8, 8, 8, 64)\n",
      "conv2 shape: (None, 8, 8, 8, 128)\n",
      "conv2 shape: (None, 8, 8, 8, 128)\n",
      "pool2 shape: (None, 4, 4, 4, 128)\n",
      "conv3 shape: (None, 4, 4, 4, 256)\n",
      "conv3 shape: (None, 4, 4, 4, 256)\n",
      "pool3 shape: (None, 2, 2, 2, 256)\n",
      "conv4 shape: (None, 2, 2, 2, 512)\n",
      "conv4 shape: (None, 2, 2, 2, 512)\n",
      "pool4 shape: (None, 1, 1, 1, 512)\n",
      "conv5 shape: (None, 1, 1, 1, 1024)\n",
      "conv5 shape: (None, 1, 1, 1, 1024)\n",
      "up6 shape: (None, 2, 2, 2, 512)\n",
      "merge6 shape: (None, 2, 2, 2, 1024)\n",
      "conv6 shape: (None, 2, 2, 2, 512)\n",
      "conv6 shape: (None, 2, 2, 2, 512)\n",
      "up7 shape: (None, 4, 4, 4, 256)\n",
      "merge7 shape: (None, 4, 4, 4, 512)\n",
      "conv7 shape: (None, 4, 4, 4, 256)\n",
      "conv7 shape: (None, 4, 4, 4, 256)\n",
      "up8 shape: (None, 8, 8, 8, 128)\n",
      "merge8 shape: (None, 8, 8, 8, 256)\n",
      "conv8 shape: (None, 8, 8, 8, 128)\n",
      "conv8 shape: (None, 8, 8, 8, 128)\n",
      "up9 shape: (None, 16, 16, 16, 64)\n",
      "merge9 shape: (None, 16, 16, 16, 128)\n",
      "conv9 shape: (None, 16, 16, 16, 64)\n",
      "conv9 shape: (None, 16, 16, 16, 64)\n",
      "conv9 shape: (None, 16, 16, 16, 2)\n",
      "sobel_layer shape: (None, 16, 16, 16, 2)\n",
      "conv10 shape: (None, 16, 16, 16, 1)\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling SobelLayer.call().\n\n\u001b[1mCannot reshape a tensor with 81 elements to shape [3,3,3,2,1] (54 elements) for '{{node functional_5_1/sobel_layer_6_1/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](functional_5_1/sobel_layer_6_1/Const, functional_5_1/sobel_layer_6_1/Reshape/shape)' with input shapes: [3,3,3,3], [5] and with input tensors computed as partial shapes: input[1] = [3,3,3,2,1].\u001b[0m\n\nArguments received by SobelLayer.call():\n  • inputs=tf.Tensor(shape=(None, 16, 16, 16, 2), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 259\u001b[0m\n\u001b[0;32m    256\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    260\u001b[0m     x_train, y_train, \n\u001b[0;32m    261\u001b[0m     validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m    262\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \n\u001b[0;32m    263\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m    264\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[lr_scheduler, early_stopping]\n\u001b[0;32m    265\u001b[0m )\n\u001b[0;32m    267\u001b[0m \u001b[38;5;66;03m# 评估模型\u001b[39;00m\n\u001b[0;32m    268\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[10], line 126\u001b[0m, in \u001b[0;36mSobelLayer.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    102\u001b[0m sobel_y \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([[[[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]],\n\u001b[0;32m    103\u001b[0m                         [[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    104\u001b[0m                         [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m]]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m                         [[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    112\u001b[0m                         [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m]]]], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    114\u001b[0m sobel_z \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([[[[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m]],\n\u001b[0;32m    115\u001b[0m                         [[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m]],\n\u001b[0;32m    116\u001b[0m                         [[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m]]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m                         [[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m]],\n\u001b[0;32m    124\u001b[0m                         [[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m]]]], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m--> 126\u001b[0m sobel_x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(sobel_x, [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    127\u001b[0m sobel_y \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(sobel_y, [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    128\u001b[0m sobel_z \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(sobel_z, [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling SobelLayer.call().\n\n\u001b[1mCannot reshape a tensor with 81 elements to shape [3,3,3,2,1] (54 elements) for '{{node functional_5_1/sobel_layer_6_1/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](functional_5_1/sobel_layer_6_1/Const, functional_5_1/sobel_layer_6_1/Reshape/shape)' with input shapes: [3,3,3,3], [5] and with input tensors computed as partial shapes: input[1] = [3,3,3,2,1].\u001b[0m\n\nArguments received by SobelLayer.call():\n  • inputs=tf.Tensor(shape=(None, 16, 16, 16, 2), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 读取点云数据和标签\n",
    "def load_labeled_point_cloud(file_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 4:\n",
    "                x, y, z, label = map(float, parts)\n",
    "                data.append([x, y, z])\n",
    "                labels.append(int(label))\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "def create_voxel_grid(data, labels, grid_size):\n",
    "    if data.size == 0 or labels.size == 0:\n",
    "        print(f\"Warning: Empty data or labels array. data.size={data.size}, labels.size={labels.size}\")\n",
    "        return None, None\n",
    "\n",
    "    grid = np.zeros((grid_size, grid_size, grid_size))\n",
    "    label_grid = np.zeros((grid_size, grid_size, grid_size))\n",
    "\n",
    "    min_coords = np.min(data, axis=0)\n",
    "    max_coords = np.max(data, axis=0)\n",
    "    voxel_dim = (max_coords - min_coords) / grid_size\n",
    "\n",
    "    for i, point in enumerate(data):\n",
    "        voxel = ((point - min_coords) / voxel_dim).astype(int)\n",
    "        voxel = np.clip(voxel, 0, grid_size-1)  # Ensure indices are within bounds\n",
    "        grid[voxel[0], voxel[1], voxel[2]] = 1\n",
    "        label_grid[voxel[0], voxel[1], voxel[2]] = labels[i] - 1  # Convert labels 1 and 2 to 0 and 1\n",
    "\n",
    "    return grid, label_grid\n",
    "\n",
    "def load_data_from_directory(data_dir, grid_size=16):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith(\"_labeled.txt\"):\n",
    "            file_path = os.path.join(data_dir, file)\n",
    "            data, labels = load_labeled_point_cloud(file_path)\n",
    "            if data.size == 0 or labels.size == 0:\n",
    "                print(f\"Skipping empty file: {file_path}\")\n",
    "                continue\n",
    "            voxel_grid, label_grid = create_voxel_grid(data, labels, grid_size)\n",
    "            if voxel_grid is not None and label_grid is not None:\n",
    "                # 应用数据增强\n",
    "                voxel_grid = augment_voxel_grid(voxel_grid)\n",
    "                x_data.append(voxel_grid)\n",
    "                y_data.append(label_grid)\n",
    "    x_data = np.expand_dims(np.array(x_data), axis=-1)\n",
    "    y_data = np.expand_dims(np.array(y_data), axis=-1)\n",
    "    return x_data, y_data\n",
    "\n",
    "# 数据增强\n",
    "def augment_voxel_grid(voxel_grid):\n",
    "    \"\"\"\n",
    "    对体素网格进行随机旋转和翻转。\n",
    "    \"\"\"\n",
    "    # 随机旋转\n",
    "    angle = np.random.uniform(0, 360)\n",
    "    voxel_grid = np.rot90(voxel_grid, k=int(angle // 90), axes=(0, 1))\n",
    "    \n",
    "    # 随机翻转\n",
    "    if np.random.rand() > 0.5:\n",
    "        voxel_grid = np.flip(voxel_grid, axis=0)\n",
    "    if np.random.rand() > 0.5:\n",
    "        voxel_grid = np.flip(voxel_grid, axis=1)\n",
    "    if np.random.rand() > 0.5:\n",
    "        voxel_grid = np.flip(voxel_grid, axis=2)\n",
    "    \n",
    "    return voxel_grid\n",
    "\n",
    "\n",
    "# 定义sobel层\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class SobelLayer(Layer):\n",
    "    def __init__(self):\n",
    "        super(SobelLayer, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # 3D Sobel kernels\n",
    "        sobel_x = tf.constant([[[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]],\n",
    "                                [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]],\n",
    "                                [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]],\n",
    "\n",
    "                               [[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]],\n",
    "                                [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]],\n",
    "                                [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]],\n",
    "\n",
    "                               [[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]],\n",
    "                                [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]],\n",
    "                                [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]]], dtype=tf.float32)\n",
    "\n",
    "        sobel_y = tf.constant([[[[-1, -2, -1], [-1, -2, -1], [-1, -2, -1]],\n",
    "                                [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "                                [[1, 2, 1], [1, 2, 1], [1, 2, 1]]],\n",
    "\n",
    "                               [[[-1, -2, -1], [-1, -2, -1], [-1, -2, -1]],\n",
    "                                [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "                                [[1, 2, 1], [1, 2, 1], [1, 2, 1]]],\n",
    "\n",
    "                               [[[-1, -2, -1], [-1, -2, -1], [-1, -2, -1]],\n",
    "                                [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "                                [[1, 2, 1], [1, 2, 1], [1, 2, 1]]]], dtype=tf.float32)\n",
    "\n",
    "        sobel_z = tf.constant([[[[-1, -2, -1], [0, 0, 0], [1, 2, 1]],\n",
    "                                [[-1, -2, -1], [0, 0, 0], [1, 2, 1]],\n",
    "                                [[-1, -2, -1], [0, 0, 0], [1, 2, 1]]],\n",
    "\n",
    "                               [[[-1, -2, -1], [0, 0, 0], [1, 2, 1]],\n",
    "                                [[-1, -2, -1], [0, 0, 0], [1, 2, 1]],\n",
    "                                [[-1, -2, -1], [0, 0, 0], [1, 2, 1]]],\n",
    "\n",
    "                               [[[-1, -2, -1], [0, 0, 0], [1, 2, 1]],\n",
    "                                [[-1, -2, -1], [0, 0, 0], [1, 2, 1]],\n",
    "                                [[-1, -2, -1], [0, 0, 0], [1, 2, 1]]]], dtype=tf.float32)\n",
    "\n",
    "        sobel_x = tf.reshape(sobel_x, [3, 3, 3, 2, 1])\n",
    "        sobel_y = tf.reshape(sobel_y, [3, 3, 3, 2, 1])\n",
    "        sobel_z = tf.reshape(sobel_z, [3, 3, 3, 2, 1])\n",
    "\n",
    "        gx = tf.nn.conv3d(inputs, sobel_x, strides=[1, 1, 1, 1, 1], padding='SAME')\n",
    "        gy = tf.nn.conv3d(inputs, sobel_y, strides=[1, 1, 1, 1, 1], padding='SAME')\n",
    "        gz = tf.nn.conv3d(inputs, sobel_z, strides=[1, 1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "        sobel_magnitude = tf.sqrt(tf.square(gx) + tf.square(gy) + tf.square(gz))\n",
    "        return sobel_magnitude\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "\n",
    "# 定义UNET模型\n",
    "def unet_3d(input_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    print(f\"Input shape: {inputs.shape}\")\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = tf.keras.layers.Conv3D(64, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(inputs)\n",
    "    print(f\"conv1 shape: {conv1.shape}\")\n",
    "    conv1 = tf.keras.layers.Conv3D(64, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv1)\n",
    "    print(f\"conv1 shape: {conv1.shape}\")\n",
    "    pool1 = tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(conv1)\n",
    "    print(f\"pool1 shape: {pool1.shape}\")\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv3D(128, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pool1)\n",
    "    print(f\"conv2 shape: {conv2.shape}\")\n",
    "    conv2 = tf.keras.layers.Conv3D(128, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv2)\n",
    "    print(f\"conv2 shape: {conv2.shape}\")\n",
    "    pool2 = tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(conv2)\n",
    "    print(f\"pool2 shape: {pool2.shape}\")\n",
    "\n",
    "    conv3 = tf.keras.layers.Conv3D(256, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pool2)\n",
    "    print(f\"conv3 shape: {conv3.shape}\")\n",
    "    conv3 = tf.keras.layers.Conv3D(256, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv3)\n",
    "    print(f\"conv3 shape: {conv3.shape}\")\n",
    "    pool3 = tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(conv3)\n",
    "    print(f\"pool3 shape: {pool3.shape}\")\n",
    "\n",
    "    conv4 = tf.keras.layers.Conv3D(512, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pool3)\n",
    "    print(f\"conv4 shape: {conv4.shape}\")\n",
    "    conv4 = tf.keras.layers.Conv3D(512, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv4)\n",
    "    print(f\"conv4 shape: {conv4.shape}\")\n",
    "    pool4 = tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(conv4)\n",
    "    print(f\"pool4 shape: {pool4.shape}\")\n",
    "\n",
    "    conv5 = tf.keras.layers.Conv3D(1024, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pool4)\n",
    "    print(f\"conv5 shape: {conv5.shape}\")\n",
    "    conv5 = tf.keras.layers.Conv3D(1024, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv5)\n",
    "    print(f\"conv5 shape: {conv5.shape}\")\n",
    "\n",
    "    # Decoder\n",
    "    up6 = tf.keras.layers.Conv3D(512, 2, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(tf.keras.layers.UpSampling3D(size=(2, 2, 2))(conv5))\n",
    "    print(f\"up6 shape: {up6.shape}\")\n",
    "    merge6 = tf.keras.layers.concatenate([conv4, up6], axis=4)\n",
    "    print(f\"merge6 shape: {merge6.shape}\")\n",
    "    conv6 = tf.keras.layers.Conv3D(512, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(merge6)\n",
    "    print(f\"conv6 shape: {conv6.shape}\")\n",
    "    conv6 = tf.keras.layers.Conv3D(512, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv6)\n",
    "    print(f\"conv6 shape: {conv6.shape}\")\n",
    "\n",
    "    up7 = tf.keras.layers.Conv3D(256, 2, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(tf.keras.layers.UpSampling3D(size=(2, 2, 2))(conv6))\n",
    "    print(f\"up7 shape: {up7.shape}\")\n",
    "    merge7 = tf.keras.layers.concatenate([conv3, up7], axis=4)\n",
    "    print(f\"merge7 shape: {merge7.shape}\")\n",
    "    conv7 = tf.keras.layers.Conv3D(256, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(merge7)\n",
    "    print(f\"conv7 shape: {conv7.shape}\")\n",
    "    conv7 = tf.keras.layers.Conv3D(256, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv7)\n",
    "    print(f\"conv7 shape: {conv7.shape}\")\n",
    "\n",
    "    up8 = tf.keras.layers.Conv3D(128, 2, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(tf.keras.layers.UpSampling3D(size=(2, 2, 2))(conv7))\n",
    "    print(f\"up8 shape: {up8.shape}\")\n",
    "    merge8 = tf.keras.layers.concatenate([conv2, up8], axis=4)\n",
    "    print(f\"merge8 shape: {merge8.shape}\")\n",
    "    conv8 = tf.keras.layers.Conv3D(128, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(merge8)\n",
    "    print(f\"conv8 shape: {conv8.shape}\")\n",
    "    conv8 = tf.keras.layers.Conv3D(128, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv8)\n",
    "    print(f\"conv8 shape: {conv8.shape}\")\n",
    "\n",
    "    up9 = tf.keras.layers.Conv3D(64, 2, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(tf.keras.layers.UpSampling3D(size=(2, 2, 2))(conv8))\n",
    "    print(f\"up9 shape: {up9.shape}\")\n",
    "    merge9 = tf.keras.layers.concatenate([conv1, up9], axis=4)\n",
    "    print(f\"merge9 shape: {merge9.shape}\")\n",
    "    conv9 = tf.keras.layers.Conv3D(64, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(merge9)\n",
    "    print(f\"conv9 shape: {conv9.shape}\")\n",
    "    conv9 = tf.keras.layers.Conv3D(64, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv9)\n",
    "    print(f\"conv9 shape: {conv9.shape}\")\n",
    "    conv9 = tf.keras.layers.Conv3D(2, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv9)\n",
    "    print(f\"conv9 shape: {conv9.shape}\")\n",
    "\n",
    "    # 在模型的末尾添加 Sobel 层\n",
    "    sobel_layer = SobelLayer()(conv9)\n",
    "    print(f\"sobel_layer shape: {sobel_layer.shape}\")\n",
    "\n",
    "    conv10 = tf.keras.layers.Conv3D(1, 1, activation='sigmoid')(sobel_layer)\n",
    "    print(f\"conv10 shape: {conv10.shape}\")\n",
    "\n",
    "    model = tf.keras.Model(inputs, conv10)\n",
    "    return model\n",
    "\n",
    "# 设置数据路径\n",
    "data_dir = '重新处理后的数据_手工筛选'\n",
    "\n",
    "# 加载数据\n",
    "x_data, y_data = load_data_from_directory(data_dir, grid_size=16)\n",
    "\n",
    "# 分割数据集\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# 定义学习率调度函数\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return float(lr)\n",
    "    else:\n",
    "        return float(lr * tf.math.exp(-0.2).numpy())\n",
    "\n",
    "# 定义学习率调度回调和早停回调\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# 定义模型\n",
    "input_shape = (16, 16, 16, 1)\n",
    "model = unet_3d(input_shape)\n",
    "\n",
    "# 编译模型\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, clipnorm=1.0)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "history = model.fit(\n",
    "    x_train, y_train, \n",
    "    validation_split=0.1,\n",
    "    epochs=2, \n",
    "    batch_size=10,\n",
    "    callbacks=[lr_scheduler, early_stopping]\n",
    ")\n",
    "\n",
    "# 评估模型\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# 绘制训练过程中的损失和精确度\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    绘制训练过程中的损失和精确度。\n",
    "    \"\"\"\n",
    "    # 绘制损失\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # 绘制精确度\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 绘制训练历史\n",
    "plot_training_history(history)\n",
    "\n",
    "# 保存模型\n",
    "model_save_path = 'July27模型/原始UNET_Sobel.h5'\n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875df335",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "模型用于预测，以及结果可视化\n",
    "'''\n",
    "\n",
    "# 开启交互旋转\n",
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import tensorflow as tf\n",
    "from scipy.spatial import KDTree\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "# 加载obj文件\n",
    "def load_obj_file(file_path):\n",
    "    vertices = []\n",
    "    faces = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                if line.startswith('v '):\n",
    "                    parts = line.strip().split()\n",
    "                    vertex = [float(parts[1]), float(parts[2]), float(parts[3])]\n",
    "                    vertices.append(vertex)\n",
    "                elif line.startswith('f '):\n",
    "                    parts = line.strip().split()\n",
    "                    face = [int(p.split('/')[0]) - 1 for p in parts[1:]]\n",
    "                    faces.append(face)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return vertices, faces\n",
    "\n",
    "# 将顶点平移至包围盒中心\n",
    "def center_vertices(vertices):\n",
    "    vertices_array = np.array(vertices)\n",
    "    min_coords = vertices_array.min(axis=0)\n",
    "    max_coords = vertices_array.max(axis=0)\n",
    "    center = (min_coords + max_coords) / 2\n",
    "    centered_vertices = vertices_array - center\n",
    "    return centered_vertices.tolist()\n",
    "\n",
    "# 点云转体素网格\n",
    "def create_voxel_grid(data, grid_size):\n",
    "    grid = np.zeros((grid_size, grid_size, grid_size))\n",
    "    min_coords = np.min(data, axis=0)\n",
    "    max_coords = np.max(data, axis=0)\n",
    "    voxel_dim = (max_coords - min_coords) / grid_size\n",
    "\n",
    "    for i, point in enumerate(data):\n",
    "        voxel = ((point - min_coords) / voxel_dim).astype(int)\n",
    "        voxel = np.clip(voxel, 0, grid_size-1)  # Ensure indices are within bounds\n",
    "        grid[voxel[0], voxel[1], voxel[2]] = 1\n",
    "\n",
    "    return grid, min_coords, voxel_dim\n",
    "\n",
    "# 旋转点云\n",
    "def rotate_points(points, angles):\n",
    "    x_angle, y_angle, z_angle = angles\n",
    "    Rx = np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, np.cos(x_angle), -np.sin(x_angle)],\n",
    "        [0, np.sin(x_angle), np.cos(x_angle)]\n",
    "    ])\n",
    "    Ry = np.array([\n",
    "        [np.cos(y_angle), 0, np.sin(y_angle)],\n",
    "        [0, 1, 0],\n",
    "        [-np.sin(y_angle), 0, np.cos(y_angle)]\n",
    "    ])\n",
    "    Rz = np.array([\n",
    "        [np.cos(z_angle), -np.sin(z_angle), 0],\n",
    "        [np.sin(z_angle), np.cos(z_angle), 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    R = Rz @ Ry @ Rx\n",
    "    return points @ R.T\n",
    "\n",
    "# 从训练好的模型获取标签\n",
    "def get_labels_from_model(model, voxel_grid):\n",
    "    voxel_grid = np.expand_dims(voxel_grid, axis=0)  # Add batch dimension\n",
    "    voxel_grid = np.expand_dims(voxel_grid, axis=-1)  # Add channel dimension\n",
    "    predictions = model.predict(voxel_grid)\n",
    "    labels = (predictions > 0.5).astype(int)\n",
    "    return labels.reshape(voxel_grid.shape[1], voxel_grid.shape[2], voxel_grid.shape[3])\n",
    "\n",
    "# 应用预测标签到原始点云\n",
    "def apply_labels_to_point_cloud(data, predicted_labels, min_coords, voxel_dim, grid_size):\n",
    "    labels = np.zeros(len(data))\n",
    "    for i, point in enumerate(data):\n",
    "        voxel = ((point - min_coords) / voxel_dim).astype(int)\n",
    "        voxel = np.clip(voxel, 0, grid_size-1)  # Ensure indices are within bounds\n",
    "        labels[i] = predicted_labels[voxel[0], voxel[1], voxel[2]]\n",
    "    return labels\n",
    "\n",
    "# 识别并合并分割线段\n",
    "def find_boundary_edges(vertices, faces, labels):\n",
    "    edges = defaultdict(list)\n",
    "    for face in faces:\n",
    "        for (v1, v2) in combinations(face, 2):\n",
    "            if labels[v1] != labels[v2]:\n",
    "                edges[tuple(sorted([v1, v2]))].append(face)\n",
    "    \n",
    "    # Create an adjacency list for edges\n",
    "    adjacency_list = defaultdict(list)\n",
    "    for (v1, v2), faces in edges.items():\n",
    "        adjacency_list[v1].append(v2)\n",
    "        adjacency_list[v2].append(v1)\n",
    "    \n",
    "    # Find all connected components of edges\n",
    "    visited = set()\n",
    "    boundary_lines = []\n",
    "    for vertex in adjacency_list:\n",
    "        if vertex not in visited:\n",
    "            stack = [vertex]\n",
    "            boundary_line = []\n",
    "            while stack:\n",
    "                current = stack.pop()\n",
    "                if current not in visited:\n",
    "                    visited.add(current)\n",
    "                    boundary_line.append(current)\n",
    "                    for neighbor in adjacency_list[current]:\n",
    "                        if neighbor not in visited:\n",
    "                            stack.append(neighbor)\n",
    "            boundary_lines.append(boundary_line)\n",
    "    \n",
    "    # Calculate the length of each boundary line and sort by length\n",
    "    boundary_lines = sorted(boundary_lines, key=lambda line: sum(np.linalg.norm(vertices[line[i]] - vertices[line[i + 1]]) for i in range(len(line) - 1)), reverse=True)\n",
    "    return boundary_lines\n",
    "\n",
    "# 绘制带有分类标签的点云和分界线\n",
    "def plot_surface_with_marks(vertices, faces, labels, part1, part2, view_angles=(30, 30), angles=(0, 0, 0)):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    vertices = np.array(vertices)\n",
    "    faces = np.array(faces)\n",
    "\n",
    "    # Apply rotation\n",
    "    vertices = rotate_points(vertices, angles)\n",
    "    \n",
    "    x, y, z = vertices.T\n",
    "\n",
    "    try:\n",
    "        # 标记分割的点云部分\n",
    "        part1_faces = [face for face in faces if sum(labels[vertex] == 0 for vertex in face) > 1]\n",
    "        part2_faces = [face for face in faces if sum(labels[vertex] == 1 for vertex in face) > 1]\n",
    "        \n",
    "        part1_faces = np.array(part1_faces)\n",
    "        part2_faces = np.array(part2_faces)\n",
    "        \n",
    "        if len(part1_faces) > 0:\n",
    "            ax.plot_trisurf(vertices[:, 0], vertices[:, 1], vertices[:, 2], triangles=part1_faces, color='cornflowerblue', alpha=0.6)\n",
    "        if len(part2_faces) > 0:\n",
    "            ax.plot_trisurf(vertices[:, 0], vertices[:, 1], vertices[:, 2], triangles=part2_faces, color='honeydew', alpha=0.6)\n",
    "\n",
    "        # 找到边界线并绘制最长的边界线\n",
    "        boundary_lines = find_boundary_edges(vertices, faces, labels)\n",
    "        if boundary_lines:\n",
    "            longest_boundary_line = boundary_lines[0]\n",
    "            ax.plot(vertices[longest_boundary_line, 0], vertices[longest_boundary_line, 1], vertices[longest_boundary_line, 2], color='red')\n",
    "            \n",
    "    except ValueError as e:\n",
    "        print(f\"ValueError: {e}\")\n",
    "        return\n",
    "\n",
    "    # 设置标签和标题\n",
    "    ax.set_xlabel('X axis')\n",
    "    ax.set_ylabel('Y axis')\n",
    "    ax.set_zlabel('Z axis')\n",
    "    ax.set_title('3D Model with Segmentation and Boundary Lines')\n",
    "\n",
    "    # 确保坐标轴刻度一致\n",
    "    max_range = np.array([max(x)-min(x), max(y)-min(y), max(z)-min(z)]).max()\n",
    "    mid_x = (max(x) + min(x)) * 0.5\n",
    "    mid_y = (max(y) + min(y)) * 0.5\n",
    "    mid_z = (max(z) + min(z)) * 0.5\n",
    "    ax.set_xlim(mid_x - max_range/2, mid_x + max_range/2)\n",
    "    ax.set_ylim(mid_y - max_range/2, mid_y + max_range/2)\n",
    "    ax.set_zlim(mid_z - max_range/2, mid_z + max_range/2)\n",
    "\n",
    "    # 设置视角\n",
    "    elev, azim = view_angles\n",
    "    ax.view_init(elev=elev, azim=azim)  # Adjust these values as needed\n",
    "\n",
    "    # 确保坐标轴比例相等\n",
    "    ax.set_box_aspect([1,1,1])  # Aspect ratio is 1:1:1\n",
    "\n",
    "    # 启用交互式旋转\n",
    "    plt.show()\n",
    "\n",
    "# 加载点云数据\n",
    "obj_file_path = r'D:/李娅宁/肩台外侧点-0715/已完成预处理的原始数据/15/15_1.obj'\n",
    "vertices, faces = load_obj_file(obj_file_path)\n",
    "centered_vertices = center_vertices(vertices)\n",
    "\n",
    "# 将点云转换为体素网格\n",
    "grid_size = 16  # Ensure grid size matches model requirements\n",
    "voxel_grid, min_coords, voxel_dim = create_voxel_grid(np.array(centered_vertices), grid_size)\n",
    "\n",
    "# 使用训练好的模型进行预测\n",
    "predicted_labels = get_labels_from_model(model, voxel_grid)\n",
    "\n",
    "# 获取原始点云的预测标签\n",
    "predicted_point_labels = apply_labels_to_point_cloud(np.array(centered_vertices), predicted_labels, min_coords, voxel_dim, grid_size)\n",
    "\n",
    "# 定义分割的部分（需要根据实际情况定义）\n",
    "part1 = {i for i, label in enumerate(predicted_point_labels) if label == 0}\n",
    "part2 = {i for i, label in enumerate(predicted_point_labels) if label == 1}\n",
    "\n",
    "# 画图\n",
    "plot_surface_with_marks(centered_vertices, faces, predicted_point_labels, part1, part2, view_angles=(30, 30), angles=(np.radians(90), np.radians(-30), np.radians(30)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ec7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87baf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary损失函数在这里适用吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8d5e08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

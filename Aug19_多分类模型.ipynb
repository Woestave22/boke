{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c57e010-489b-4004-be60-8c3cecbcf570",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "第一版多分类模型\n",
    "加载数据后需要检查点云的连通性\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479c48f2-d80f-404a-bf2f-e23931694de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "多分类预处理\n",
    "\n",
    "0.读取数据\n",
    "1.删除G2中的孤岛\n",
    "2.找到节点\n",
    "3.找到不重复的路径\n",
    "4.样条插值\n",
    "5.合并插值线\n",
    "6.分割\n",
    "7.投票修复\n",
    "\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd0357-76a6-47ef-81e6-da1ef3ac6739",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "加载数据\n",
    "'''\n",
    "\n",
    "# 读取点云数据和标签\n",
    "def load_labeled_point_cloud(file_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 4:\n",
    "                x, y, z, label = map(float, parts)\n",
    "                data.append([x, y, z])\n",
    "                labels.append(int(label))\n",
    "    return np.array(data), np.array(labels)\n",
    "'''\n",
    "# 读取点云数据和标签\n",
    "def load_labeled_point_cloud(file_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # 跳过第一行表头\n",
    "        next(file)\n",
    "        for line in file:\n",
    "            # 按照逗号分隔每行数据\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) == 4:\n",
    "                x, y, z, label = map(float, parts)\n",
    "                data.append([x, y, z])\n",
    "                labels.append(int(label))\n",
    "    return np.array(data), np.array(labels)\n",
    "'''\n",
    "# 创建带标签的体素网格\n",
    "def create_labeled_voxel_grid(data, labels, grid_size):\n",
    "    if data.size == 0 or labels.size == 0:\n",
    "        print(f\"Warning: Empty data or labels array. data.size={data.size}, labels.size={labels.size}\")\n",
    "        return None, None\n",
    "\n",
    "    grid = np.zeros((grid_size, grid_size, grid_size))\n",
    "    label_grid = np.zeros((grid_size, grid_size, grid_size), dtype=int)  # Initialize with 0 to indicate empty space\n",
    "\n",
    "    min_coords = np.min(data, axis=0)\n",
    "    max_coords = np.max(data, axis=0)\n",
    "    voxel_dim = (max_coords - min_coords) / grid_size\n",
    "\n",
    "    voxel_label_dict = {}\n",
    "\n",
    "    for i, point in enumerate(data):\n",
    "        voxel = ((point - min_coords) / voxel_dim).astype(int)\n",
    "        voxel = np.clip(voxel, 0, grid_size-1)  # Ensure indices are within bounds\n",
    "        voxel_key = tuple(voxel)\n",
    "        \n",
    "        if voxel_key not in voxel_label_dict:\n",
    "            voxel_label_dict[voxel_key] = []\n",
    "        voxel_label_dict[voxel_key].append(labels[i])\n",
    "\n",
    "    for voxel_key, label_list in voxel_label_dict.items():\n",
    "        # 计算众数\n",
    "        mode_result = mode(label_list)\n",
    "        \n",
    "        # 获取众数标签\n",
    "        most_common_label = mode_result.mode  # mode_result.mode 是一个标量\n",
    "        \n",
    "        grid[voxel_key] = 1\n",
    "        label_grid[voxel_key] = most_common_label\n",
    "\n",
    "    return grid, label_grid\n",
    "\n",
    "\n",
    "# 从文件夹中读取数据并加工成体素网格\n",
    "def load_data_from_directory(data_dir, grid_size=16):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith(\"_labeled.txt\"):\n",
    "            file_path = os.path.join(data_dir, file)\n",
    "            data, labels = load_labeled_point_cloud(file_path)\n",
    "            if data.size == 0 or labels.size == 0:\n",
    "                print(f\"Skipping empty file: {file_path}\")\n",
    "                continue\n",
    "            voxel_grid, label_grid = create_labeled_voxel_grid(data, labels, grid_size)\n",
    "            if voxel_grid is not None and label_grid is not None:\n",
    "                x_data.append(voxel_grid)\n",
    "                y_data.append(label_grid)\n",
    "    x_data = np.expand_dims(np.array(x_data), axis=-1)\n",
    "    y_data = np.expand_dims(np.array(y_data), axis=-1)\n",
    "    return x_data, y_data\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "x_data, y_data = load_data_from_directory(data_dir, grid_size=16)\n",
    "\n",
    "# 确保数据数组不为空\n",
    "if x_data.size > 0:\n",
    "    # 确保数据归一化\n",
    "    x_data = x_data / np.max(x_data)\n",
    "else:\n",
    "    raise ValueError(\"x_data 数组为空，无法进行归一化处理。\")\n",
    "\n",
    "# 确保标签是0-9的整数\n",
    "y_data = y_data.astype(np.int32)\n",
    "\n",
    "# 划分数据集\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_data, y_data, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2530cb-0b05-4988-a41e-e454cb08f722",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "训练模型\n",
    "'''\n",
    "\n",
    "# 绘制训练过程中的损失和精确度\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    绘制训练过程中的损失和精确度。\n",
    "    \"\"\"\n",
    "    # 绘制损失\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # 绘制精确度\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 定义UNET模型\n",
    "def unet_3d(input_shape):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = tf.keras.layers.Conv3D(64, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(inputs)\n",
    "    conv1 = tf.keras.layers.Conv3D(64, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv1)\n",
    "    pool1 = tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(conv1)\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv3D(128, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pool1)\n",
    "    conv2 = tf.keras.layers.Conv3D(128, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv2)\n",
    "    pool2 = tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(conv2)\n",
    "\n",
    "    conv3 = tf.keras.layers.Conv3D(256, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pool2)\n",
    "    conv3 = tf.keras.layers.Conv3D(256, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv3)\n",
    "    pool3 = tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(conv3)\n",
    "\n",
    "    # Bottleneck\n",
    "    conv4 = tf.keras.layers.Conv3D(512, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(pool3)\n",
    "    conv4 = tf.keras.layers.Conv3D(512, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv4)\n",
    "\n",
    "    # Decoder\n",
    "    up5 = tf.keras.layers.Conv3DTranspose(256, 2, strides=(2, 2, 2), padding='same')(conv4)\n",
    "    concat5 = tf.keras.layers.concatenate([up5, conv3], axis=-1)\n",
    "    conv5 = tf.keras.layers.Conv3D(256, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(concat5)\n",
    "    conv5 = tf.keras.layers.Conv3D(256, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv5)\n",
    "\n",
    "    up6 = tf.keras.layers.Conv3DTranspose(128, 2, strides=(2, 2, 2), padding='same')(conv5)\n",
    "    concat6 = tf.keras.layers.concatenate([up6, conv2], axis=-1)\n",
    "    conv6 = tf.keras.layers.Conv3D(128, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(concat6)\n",
    "    conv6 = tf.keras.layers.Conv3D(128, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv6)\n",
    "\n",
    "    up7 = tf.keras.layers.Conv3DTranspose(64, 2, strides=(2, 2, 2), padding='same')(conv6)\n",
    "    concat7 = tf.keras.layers.concatenate([up7, conv1], axis=-1)\n",
    "    conv7 = tf.keras.layers.Conv3D(64, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(concat7)\n",
    "    conv7 = tf.keras.layers.Conv3D(64, 3, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(conv7)\n",
    "\n",
    "    # 修改输出层，适应3个类别\n",
    "    outputs = tf.keras.layers.Conv3D(9, 1, activation='softmax')(conv7)  # 9 channels for three-class classification\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "# 定义各种损失函数\n",
    "# 交叉熵\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True))\n",
    "\n",
    "# Dice\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3, 4])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2, 3, 4]) + tf.reduce_sum(y_pred, axis=[1, 2, 3, 4])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    return tf.reduce_mean(1 - dice)\n",
    "\n",
    "# Focal\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        # 确保 y_true 和 y_pred 都是 float32 类型\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # 计算交叉熵\n",
    "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
    "        # 计算 focal 权重\n",
    "        focal_weight = alpha * tf.math.pow(1 - y_pred, gamma)\n",
    "        # 计算 focal loss\n",
    "        loss = focal_weight * cross_entropy\n",
    "        return tf.reduce_mean(tf.reduce_sum(loss, axis=-1))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "\n",
    "# Tversky\n",
    "def tversky_loss(y_true, y_pred, alpha=0.7, beta=0.3, smooth=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3, 4])\n",
    "    false_neg = tf.reduce_sum(y_true * (1 - y_pred), axis=[1, 2, 3, 4])\n",
    "    false_pos = tf.reduce_sum((1 - y_true) * y_pred, axis=[1, 2, 3, 4])\n",
    "    \n",
    "    tversky = (intersection + smooth) / (intersection + alpha * false_neg + beta * false_pos + smooth)\n",
    "    \n",
    "    return tf.reduce_mean(1 - tversky)\n",
    "\n",
    "\n",
    "# 组合损失函数\n",
    "@tf.function\n",
    "def combined_loss(y_true, y_pred):\n",
    "    ce_loss = cross_entropy_loss(y_true, y_pred)\n",
    "    dice = dice_loss(y_true, y_pred)\n",
    "    tversky = tversky_loss(y_true, y_pred)\n",
    "    focal = focal_loss(gamma=2.0, alpha=0.25)(y_true, y_pred)\n",
    "    \n",
    "    return ce_loss + dice + tversky + focal\n",
    "\n",
    "# 定义模型\n",
    "input_shape = (16, 16, 16, 1)\n",
    "model = unet_3d(input_shape)\n",
    "\n",
    "# 定义学习率调度函数\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return float(lr)\n",
    "    else:\n",
    "        return float(lr * tf.math.exp(decay_rate).numpy())\n",
    "\n",
    "# 定义学习率调度回调和早停回调\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate), \n",
    "              loss=combined_loss,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "history = model.fit(\n",
    "    x_train, y_train, \n",
    "    validation_split=0.1,\n",
    "    epochs=set_epochs, \n",
    "    batch_size=5,\n",
    "    callbacks=[lr_scheduler, early_stopping]\n",
    ")\n",
    "\n",
    "# 评估模型\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# 显示炼丹过程\n",
    "plot_training_history(history)\n",
    "\n",
    "# 保存模型\n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1764a50-9aac-4f82-be8e-1cba769ee401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f740cffa-df97-4ad5-88a0-3180f01a26c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc254c40-001f-4824-adb3-b08f7554ea18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8d7af4-f0c6-4856-abdd-f02c46f73861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212200f9-1fb8-4338-80f5-b993f048c079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
